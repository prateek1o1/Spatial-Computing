{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9d6810",
   "metadata": {},
   "source": [
    "# Pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e63274",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d7e8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from scipy import signal\n",
    "import scipy.misc as misc\n",
    "\n",
    "def upsample_bilinear(image, ratio):\n",
    "    \n",
    "    h,w,c = image.shape\n",
    "    re_image = cv2.resize(image, (w*ratio, h*ratio), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return re_image\n",
    "\n",
    "def upsample_bicubic(image, ratio):\n",
    "    \n",
    "    h,w,c = image.shape\n",
    "    re_image = cv2.resize(image, (w*ratio, h*ratio), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    return re_image\n",
    "\n",
    "def upsample_interp23(image, ratio):\n",
    "\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    \n",
    "    b,r,c = image.shape\n",
    "\n",
    "    CDF23 = 2*np.array([0.5, 0.305334091185, 0, -0.072698593239, 0, 0.021809577942, 0, -0.005192756653, 0, 0.000807762146, 0, -0.000060081482])\n",
    "    d = CDF23[::-1] \n",
    "    CDF23 = np.insert(CDF23, 0, d[:-1])\n",
    "    BaseCoeff = CDF23\n",
    "    \n",
    "    first = 1\n",
    "    for z in range(1,np.int(np.log2(ratio))+1):\n",
    "        I1LRU = np.zeros((b, 2**z*r, 2**z*c))\n",
    "        if first:\n",
    "            I1LRU[:, 1:I1LRU.shape[1]:2, 1:I1LRU.shape[2]:2]=image\n",
    "            first = 0\n",
    "        else:\n",
    "            I1LRU[:,0:I1LRU.shape[1]:2,0:I1LRU.shape[2]:2]=image\n",
    "        \n",
    "        for ii in range(0,b):\n",
    "            t = I1LRU[ii,:,:]\n",
    "            for j in range(0,t.shape[0]):\n",
    "                t[j,:]=ndimage.correlate(t[j,:],BaseCoeff,mode='wrap')\n",
    "            for k in range(0,t.shape[1]):\n",
    "                t[:,k]=ndimage.correlate(t[:,k],BaseCoeff,mode='wrap')\n",
    "            I1LRU[ii,:,:]=t\n",
    "        image = I1LRU\n",
    "        \n",
    "    re_image=np.transpose(I1LRU, (1, 2, 0))\n",
    "        \n",
    "    return re_image\n",
    "\n",
    "def upsample_mat_interp23(image, ratio=4):\n",
    "    \n",
    "    '''2 pixel shift compare with original matlab version'''\n",
    "    \n",
    "    shift=2\n",
    "        \n",
    "    h,w,c = image.shape\n",
    "    \n",
    "    basecoeff = np.array([[-4.63495665e-03, -3.63442646e-03,  3.84904063e-18,\n",
    "     5.76678319e-03,  1.08358664e-02,  1.01980790e-02,\n",
    "    -9.31747402e-18, -1.75033181e-02, -3.17660068e-02,\n",
    "    -2.84531643e-02,  1.85181518e-17,  4.42450253e-02,\n",
    "     7.71733386e-02,  6.70554910e-02, -2.85299239e-17,\n",
    "    -1.01548683e-01, -1.78708388e-01, -1.60004642e-01,\n",
    "     3.61741232e-17,  2.87940558e-01,  6.25431459e-01,\n",
    "     8.97067600e-01,  1.00107877e+00,  8.97067600e-01,\n",
    "     6.25431459e-01,  2.87940558e-01,  3.61741232e-17,\n",
    "    -1.60004642e-01, -1.78708388e-01, -1.01548683e-01,\n",
    "    -2.85299239e-17,  6.70554910e-02,  7.71733386e-02,\n",
    "     4.42450253e-02,  1.85181518e-17, -2.84531643e-02,\n",
    "    -3.17660068e-02, -1.75033181e-02, -9.31747402e-18,\n",
    "     1.01980790e-02,  1.08358664e-02,  5.76678319e-03,\n",
    "     3.84904063e-18, -3.63442646e-03, -4.63495665e-03]])\n",
    "    \n",
    "    coeff = np.dot(basecoeff.T, basecoeff)\n",
    "    \n",
    "    I1LRU = np.zeros((ratio*h, ratio*w, c))\n",
    "    \n",
    "    I1LRU[shift::ratio, shift::ratio, :]=image\n",
    "    \n",
    "    for i in range(c):\n",
    "        temp = I1LRU[:, :, i]\n",
    "        temp = ndimage.convolve(temp, coeff, mode='wrap')\n",
    "        I1LRU[:, :, i]=temp\n",
    "        \n",
    "    return I1LRU\n",
    "\n",
    "def gaussian2d (N, std):\n",
    "    \n",
    "    t=np.arange(-(N-1)/2,(N+2)/2)\n",
    "    t1,t2=np.meshgrid(t,t)\n",
    "    std=np.double(std)\n",
    "    w = np.exp(-0.5*(t1/std)**2)*np.exp(-0.5*(t2/std)**2) \n",
    "    return w\n",
    "    \n",
    "def kaiser2d (N, beta):\n",
    "    \n",
    "    t=np.arange(-(N-1)/2,(N+2)/2)/np.double(N-1)\n",
    "    t1,t2=np.meshgrid(t,t)\n",
    "    t12=np.sqrt(t1*t1+t2*t2)\n",
    "    w1=np.kaiser(N,beta)\n",
    "    w=np.interp(t12,t,w1)\n",
    "    w[t12>t[-1]]=0\n",
    "    w[t12<t[0]]=0\n",
    "    \n",
    "    return w\n",
    "\n",
    "def fir_filter_wind(Hd,w):\n",
    "    \"\"\"\n",
    "\tcompute fir filter with window method\n",
    "\tHd: \tdesired freqeuncy response (2D)\n",
    "\tw: \t\twindow (2D)\n",
    "\t\"\"\"\n",
    "\t\n",
    "    hd=np.rot90(np.fft.fftshift(np.rot90(Hd,2)),2)\n",
    "    h=np.fft.fftshift(np.fft.ifft2(hd))\n",
    "    h=np.rot90(h,2)\n",
    "    h=h*w\n",
    "    h=h/np.sum(h)\n",
    "    \n",
    "    return h\n",
    "\n",
    "def downgrade_images(I_MS, I_PAN, ratio, sensor=None):\n",
    "    \"\"\"\n",
    "    downgrade MS and PAN by a ratio factor with given sensor's gains\n",
    "    \"\"\"\n",
    "    I_MS=np.double(I_MS)\n",
    "    I_PAN=np.double(I_PAN)\n",
    "    \n",
    "    I_MS = np.transpose(I_MS, (2, 0, 1))\n",
    "    I_PAN = np.squeeze(I_PAN)\n",
    "    \n",
    "    ratio=np.double(ratio)\n",
    "    flag_PAN_MTF=0\n",
    "    \n",
    "    if sensor=='QB':\n",
    "        flag_resize_new = 2\n",
    "        GNyq = np.asarray([0.34, 0.32, 0.30, 0.22],dtype='float32')    # Band Order: B,G,R,NIR\n",
    "        GNyqPan = 0.15\n",
    "    elif sensor=='IKONOS':\n",
    "        flag_resize_new = 2             #MTF usage\n",
    "        GNyq = np.asarray([0.26,0.28,0.29,0.28],dtype='float32')    # Band Order: B,G,R,NIR\n",
    "        GNyqPan = 0.17;\n",
    "    elif sensor=='GeoEye1':\n",
    "        flag_resize_new = 2             # MTF usage\n",
    "        GNyq = np.asarray([0.23,0.23,0.23,0.23],dtype='float32')    # Band Order: B,G,R,NIR\n",
    "        GNyqPan = 0.16     \n",
    "    elif sensor=='WV2':\n",
    "        flag_resize_new = 2             # MTF usage\n",
    "        GNyq = [0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.27]\n",
    "        GNyqPan = 0.11\n",
    "    elif sensor=='WV3':\n",
    "        flag_resize_new = 2             #MTF usage\n",
    "        GNyq = 0.29 * np.ones(8)\n",
    "        GNyqPan = 0.15\n",
    "    else:\n",
    "        '''the default way'''\n",
    "        flag_resize_new = 1\n",
    "    \n",
    "    '''the default downgrading method is gaussian'''\n",
    "    if flag_resize_new == 1:\n",
    "        \n",
    "#        I_MS_LP = np.zeros((I_MS.shape[0],int(np.round(I_MS.shape[1]/ratio)+ratio),int(np.round(I_MS.shape[2]/ratio)+ratio)))\n",
    "#            \n",
    "#        for idim in range(I_MS.shape[0]):\n",
    "#            imslp_pad=np.pad(I_MS[idim,:,:],int(2*ratio),'symmetric')\n",
    "#            I_MS_LP[idim,:,:]=misc.imresize(imslp_pad,1/ratio,'bicubic',mode='F')\n",
    "#            \n",
    "#        I_MS_LR = I_MS_LP[:,2:-2,2:-2]\n",
    "#       \n",
    "#        I_PAN_pad=np.pad(I_PAN,int(2*ratio),'symmetric')\n",
    "#        I_PAN_LR=misc.imresize(I_PAN_pad,1/ratio,'bicubic',mode='F')\n",
    "#        I_PAN_LR=I_PAN_LR[2:-2,2:-2]\n",
    "        \n",
    "        sig = (1/(2*(2.772587)/ratio**2))**0.5\n",
    "        kernel = np.multiply(cv2.getGaussianKernel(9, sig), cv2.getGaussianKernel(9,sig).T)\n",
    "        \n",
    "        t=[]\n",
    "        for i in range(I_MS.shape[0]):\n",
    "            temp = signal.convolve2d(I_MS[i, :, :], kernel, mode='same', boundary = 'wrap')\n",
    "            temp = temp[0::int(ratio), 0::int(ratio)]\n",
    "            temp = np.expand_dims(temp, 0)\n",
    "            t.append(temp)\n",
    "            \n",
    "        I_MS_LR = np.concatenate(t, axis=0)\n",
    "        \n",
    "        I_PAN_LR = signal.convolve2d(I_PAN, kernel, mode='same', boundary = 'wrap')\n",
    "        I_PAN_LR = I_PAN_LR[0::int(ratio), 0::int(ratio)]\n",
    "        \n",
    "    elif flag_resize_new==2:\n",
    "        \n",
    "        N=41\n",
    "        I_MS_LP=np.zeros(I_MS.shape)\n",
    "        fcut=1/ratio\n",
    "        \n",
    "        for j in range(I_MS.shape[0]):\n",
    "            #fir filter with window method\n",
    "            alpha = np.sqrt(((N-1)*(fcut/2))**2/(-2*np.log(GNyq[j])))\n",
    "            H=gaussian2d(N,alpha)\n",
    "            Hd=H/np.max(H)\n",
    "            w=kaiser2d(N,0.5)\n",
    "            h=fir_filter_wind(Hd,w)\n",
    "            I_MS_LP[j,:,:] = ndimage.filters.correlate(I_MS[j,:,:],np.real(h),mode='nearest')\n",
    "        \n",
    "        if flag_PAN_MTF==1:\n",
    "            #fir filter with window method\n",
    "            alpha = np.sqrt(((N-1)*(fcut/2))**2/(-2*np.log(GNyqPan)))\n",
    "            H=gaussian2d(N,alpha)\n",
    "            Hd=H/np.max(H)\n",
    "            h=fir_filter_wind(Hd,w)\n",
    "            I_PAN = ndimage.filters.correlate(I_PAN,np.real(h),mode='nearest')\n",
    "            I_PAN_LR=I_PAN[int(ratio/2):-1:int(ratio),int(ratio/2):-1:int(ratio)]\n",
    "            \n",
    "        else:\n",
    "            #bicubic resize\n",
    "            I_PAN_pad=np.pad(I_PAN,int(2*ratio),'symmetric')\n",
    "            I_PAN_LR=misc.imresize(I_PAN_pad,1/ratio,'bicubic',mode='F')\n",
    "            I_PAN_LR=I_PAN_LR[2:-2,2:-2]\n",
    "            \n",
    "        I_MS_LR=I_MS_LP[:,int(ratio/2):-1:int(ratio),int(ratio/2):-1:int(ratio)]     \n",
    "        \n",
    "    I_MS_LR = np.transpose(I_MS_LR, (1, 2, 0))\n",
    "    I_PAN_LR = np.expand_dims(I_PAN_LR, -1)\n",
    "    \n",
    "    return I_MS_LR,I_PAN_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a4c13",
   "metadata": {},
   "source": [
    "# Bicubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1aef27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_bicubic\n",
    "\n",
    "def Bicubic(pan, hs):\n",
    "\n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    # jsut upsample with bicubic\n",
    "    I_Bicubic = upsample_bicubic(hs, ratio)\n",
    "    \n",
    "    #adjustment\n",
    "    I_Bicubic[I_Bicubic<0]=0\n",
    "    I_Bicubic[I_Bicubic>1]=1\n",
    "    \n",
    "    return np.uint8(I_Bicubic*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58edbba",
   "metadata": {},
   "source": [
    "# Brovey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9325d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_interp23\n",
    "\n",
    "def Brovey(pan, hs):\n",
    "\n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    #upsample\n",
    "    u_hs = upsample_interp23(hs, ratio)\n",
    "    \n",
    "    I = np.mean(u_hs, axis=-1)\n",
    "    \n",
    "    image_hr = (pan-np.mean(pan))*(np.std(I, ddof=1)/np.std(pan, ddof=1))+np.mean(I)\n",
    "    image_hr = np.squeeze(image_hr)\n",
    "\n",
    "    I_Brovey=[]\n",
    "    for i in range(C):\n",
    "        temp = image_hr*u_hs[:, :, i]/(I+1e-8)\n",
    "        temp = np.expand_dims(temp, axis=-1)\n",
    "        I_Brovey.append(temp)\n",
    "        \n",
    "    I_Brovey = np.concatenate(I_Brovey, axis=-1) \n",
    "    \n",
    "    #adjustment\n",
    "    I_Brovey[I_Brovey<0]=0\n",
    "    I_Brovey[I_Brovey>1]=1\n",
    "    \n",
    "    return np.uint8(I_Brovey*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9301c",
   "metadata": {},
   "source": [
    "# CNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b1011ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "License: GNU-3.0\n",
    "Referenc: http://www.naotoyokoya.com/\n",
    "Paper References:\n",
    "    [1] N. Yokoya, T. Yairi, and A. Iwasaki, \"Coupled nonnegative matrix factorization unmixing for hyperspectral and multispectral data fusion,\" \n",
    "        IEEE Trans. Geosci. Remote Sens., vol. 50, no. 2, pp. 528-537, 2012.\n",
    "    [2] N. Yokoya, T. Yairi, and A. Iwasaki, \"Hyperspectral, multispectral, and panchromatic data fusion based on non-negative matrix factorization,\" \n",
    "        Proc. WHISPERS, Lisbon, Portugal, Jun. 6-9, 2011.\n",
    "    [3] N. Yokoya, N. Mayumi, and A. Iwasaki, \"Cross-calibration for data fusion of EO-1/Hyperion and Terra/ASTER,\" \n",
    "        IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 6, no. 2, pp. 419-426, 2013.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import erfinv\n",
    "\n",
    "def CNMF(MSI, HSI, mask=0, verbose='off',MEMs=0):\n",
    "    '''\n",
    "    COUPLED NONNEGATIVE MATRIX FACTORIZATION (CNMF)\n",
    "\n",
    "    Copyright (c) 2016 Naoto Yokoya\n",
    "    Email: yokoya@sal.rcast.u-tokyo.ac.jp\n",
    "    Update: 2016/04/01\n",
    "\n",
    "    References:\n",
    "    [1] N. Yokoya, T. Yairi, and A. Iwasaki, \"Coupled nonnegative matrix\n",
    "        factorization unmixing for hyperspectral and multispectral data fusion,\"\n",
    "        IEEE Trans. Geosci. Remote Sens., vol. 50, no. 2, pp. 528-537, 2012.\n",
    "    [2] N. Yokoya, N. Mayumi, and A. Iwasaki, \"Cross-calibration for data fusion\n",
    "        of EO-1/Hyperion and Terra/ASTER,\" IEEE J. Sel. Topics Appl. Earth Observ.\n",
    "        Remote Sens., vol. 6, no. 2, pp. 419-426, 2013.\n",
    "    [3] N. Yokoya, T. Yairi, and A. Iwasaki, \"Hyperspectral, multispectral,\n",
    "        and panchromatic data fusion based on non-negative matrix factorization,\"\n",
    "        Proc. WHISPERS, Lisbon, Portugal, Jun. 6-9, 2011.\n",
    "\n",
    "    USAGE\n",
    "        Out = CNMF_fusion(HSI,MSI,mask,verbose)\n",
    "\n",
    "    INPUT\n",
    "        HSI     : Low-spatial-resolution HS image (rows2,cols2,bands2)\n",
    "        MSI     : MS image (rows1,cols1,bands1)\n",
    "        mask    : (optional) Binary mask for processing (rows2,cols2) (0: mask, 1: image)\n",
    "        verbose : (optional) Print out processing status\n",
    "        MEMs    : (optional) Manually defined endmembers (bands2, num. of endmembers)\n",
    "\n",
    "    OUTPUT\n",
    "        Out : High-spatial-resolution HS image (rows1,cols1,bands2)\n",
    "    '''\n",
    "\n",
    "    # masking mode\n",
    "    if np.isscalar(mask):\n",
    "        masking = 0\n",
    "    else:\n",
    "        masking = 1\n",
    "\n",
    "    # image size\n",
    "    rows1 = MSI.shape[0]\n",
    "    cols1 = MSI.shape[1]\n",
    "    bands1 = MSI.shape[2]\n",
    "    rows2 = HSI.shape[0]\n",
    "    cols2 = HSI.shape[1]\n",
    "    bands2 = HSI.shape[2]\n",
    "\n",
    "    w = int(rows1/rows2)\n",
    "\n",
    "    # Estimation of R\n",
    "    if verbose == 'on':\n",
    "        print('Estimate R...')\n",
    "    R = estR(HSI,MSI,mask)\n",
    "    for b in range(bands1):\n",
    "        msi = MSI[:,:,b].reshape(rows1,cols1).copy()\n",
    "        msi = msi - R[b,-1]\n",
    "        msi[np.nonzero(msi<0)] = 0\n",
    "        MSI[:,:,b] = msi.copy()\n",
    "    R = R[:,0:bands2]\n",
    "\n",
    "    # parameters\n",
    "    th_h = 1e-8 # Threshold of change ratio in inner loop for HS unmixing\n",
    "    th_m = 1e-8 # Threshold of change ratio in inner loop for MS unmixing\n",
    "    th2 = 1e-2 # Threshold of change ratio in outer loop\n",
    "    sum2one = 2*( MSI.mean()/0.7455)**0.5 / bands1**3 # Parameter of sum to 1 constraint\n",
    "\n",
    "    if bands1 == 1:\n",
    "        I1 = 75 # Maximum iteration of inner loop\n",
    "        I2 = 1 # Maximum iteration of outer loop\n",
    "    else:\n",
    "        I1 = 200 # Maximum iteration of inner loop (200-300)\n",
    "        I2 = 1 # Maximum iteration of outer loop (1-3)\n",
    "\n",
    "    # initialization of H_hyper\n",
    "    # 0: constant (fast)\n",
    "    # 1: nonnegative least squares (slow)\n",
    "    init_mode = 0\n",
    "\n",
    "    # avoid nonnegative values\n",
    "    HSI[np.nonzero(HSI<0)] = 0\n",
    "    MSI[np.nonzero(MSI<0)] = 0\n",
    "\n",
    "    if masking == 0:\n",
    "        HSI = HSI.reshape(rows2*cols2,bands2).transpose()\n",
    "        MSI = MSI.reshape(rows1*cols1,bands1).transpose()\n",
    "    else:\n",
    "        HSI = HSI.reshape(rows2*cols2,bands2)\n",
    "        MSI = MSI.reshape(rows1*cols1,bands1)\n",
    "\n",
    "        mask2 = zoom_nn(mask,w)\n",
    "        HSI = HSI[mask.reshape(rows2*cols2)==1,:].transpose()\n",
    "        MSI = MSI[mask2.reshape(rows1*cols1)==1,:].transpose()\n",
    "\n",
    "    # manually define endmembers\n",
    "    if np.isscalar(MEMs) == False:\n",
    "        if MEMs.shape[0] == bands2 and len(MEMs.shape) == 2:\n",
    "            M_m = MEMs.shape[1]\n",
    "        else:\n",
    "            print('Please check the size of manually defined endmembers.')\n",
    "            M_m = 0\n",
    "            MEMs = 0\n",
    "    else:\n",
    "        M_m = 0\n",
    "\n",
    "    # number of endmembers\n",
    "    M_est = int(round(vd(HSI,5*10**-2)))\n",
    "    M = max([min([30,bands2]), M_est]) # M can be automatically defined, for example, by VD\n",
    "    if verbose == 'on':\n",
    "        print('Number of endmembers: ', M+M_m)\n",
    "\n",
    "    # CNMF Initializatioin\n",
    "    HSI, MSI, W_hyper, H_hyper, W_multi, H_multi, RMSE_h, RMSE_m = CNMF_init(rows1,cols1,w,M,HSI,MSI,sum2one,I1,th_h,th_m,R,init_mode,mask,verbose,MEMs)\n",
    "\n",
    "    cost = np.zeros((2,I2+1))\n",
    "    cost[0,0] = RMSE_h\n",
    "    cost[1,0] = RMSE_m\n",
    "\n",
    "    # CNMF Iteration\n",
    "    for i in range(I2):\n",
    "        W_hyper, H_hyper, W_multi1, H_multi1, W_multi2, H_multi2, RMSE_h, RMSE_m = CNMF_ite(rows1,cols1,w,M+M_m,HSI,MSI,W_hyper,H_hyper,W_multi,H_multi,I1,th_h,th_m,I2,i,R,mask,verbose)\n",
    "\n",
    "        cost[0,i+1] = RMSE_h\n",
    "        cost[1,i+1] = RMSE_m\n",
    "\n",
    "        if (cost[0,i]-cost[0,i+1])/cost[0,i]>th2 and (cost[1,i]-cost[1,i+1])/cost[1,i]>th2 and i<I2-1:\n",
    "            W_multi = W_multi2.copy()\n",
    "            H_multi = H_multi2.copy()\n",
    "        elif i == I2-1:\n",
    "            if verbose == 'on':\n",
    "                print('Max outer interation.')\n",
    "        else:\n",
    "            if verbose == 'on':\n",
    "                print('END')\n",
    "            break\n",
    "\n",
    "    if masking == 0:\n",
    "        Out = np.dot(W_hyper[0:bands2,:] , H_multi ).transpose().reshape(rows1,cols1,bands2)\n",
    "    else:\n",
    "        Out = np.zeros((rows1*cols1,bands2))\n",
    "        Out[mask2.reshape(rows1*cols1)==1,:] = np.dot(W_hyper[0:bands2,:] , H_multi ).transpose()\n",
    "        Out = Out.reshape(rows1,cols1,bands2)\n",
    "\n",
    "    #adjustment, 2020/4/13\n",
    "    Out[Out<0]=0\n",
    "    Out[Out>1]=1\n",
    "    \n",
    "    return np.uint8(Out*255)\n",
    "\n",
    "\n",
    "def CNMF_init(xdata,ydata,w,M,hyper,multi,delta,I_in,delta_h,delta_m,srf,init_mode=0,mask=0,verbose='off',MEMs=0):\n",
    "    '''\n",
    "    COUPLED NONNEGATIVE MATRIX FACTORIZATION (CNMF)\n",
    "\n",
    "    Copyright (c) 2016 Naoto Yokoya\n",
    "    Email: yokoya@sal.rcast.u-tokyo.ac.jp\n",
    "    Update: 2016/04/01\n",
    "\n",
    "    References:\n",
    "    [1] N. Yokoya, T. Yairi, and A. Iwasaki, \"Coupled nonnegative matrix\n",
    "        factorization unmixing for hyperspectral and multispectral data fusion,\"\n",
    "        IEEE Trans. Geosci. Remote Sens., vol. 50, no. 2, pp. 528-537, 2012.\n",
    "    [2] N. Yokoya, T. Yairi, and A. Iwasaki, \"Hyperspectral, multispectral,\n",
    "        and panchromatic data fusion based on non-negative matrix factorization,\"\n",
    "        Proc. WHISPERS, Lisbon, Portugal, Jun. 6-9, 2011.\n",
    "\n",
    "    This function is the initilization function of CNMF.\n",
    "\n",
    "    USAGE\n",
    "        hyper, multi, W_hyper, H_hyper, W_multi, H_multi, RMSE_h, RMSE_m =\n",
    "        CNMF_init(xdata,ydata,w,M,hyper,multi,delta,I_in,delta_h,delta_m,srf,init_mode,mask,verbose)\n",
    "\n",
    "    INPUT\n",
    "        xdata           : image height\n",
    "        ydata           : image width\n",
    "        w               : multiple difference of ground sampling distance (scalar)\n",
    "        M               : Number of endmembers\n",
    "        hyper           : Low-spatial-resolution HS image (band, xdata/w*ydata/w)\n",
    "        multi           : MS image (multi_band, xdata*ydata)\n",
    "        delta           : Parameter of sum to one constraint\n",
    "        I_in            : Maximum number of inner iteration\n",
    "        delta_h         : Parameter for HS unmixing\n",
    "        delta_m         : Parameter for MS unmixing\n",
    "        srf             : Relative specctral response function\n",
    "        init_mode       : Initialization mode (0: const, 1: nnls)\n",
    "        mask            : (optional) Binary mask for processing (xdata/w,ydata/w)\n",
    "        verbose         : (optional) Print out processing status\n",
    "        MEMs            : (optional) Manually defined endmembers (bands2, num. of endmembers)\n",
    "\n",
    "    OUTPUT\n",
    "        hyper       : Low-spatial-resolution HS image with ones (band+1, xdata/w*ydata/w)\n",
    "        multi       : MS image with ones (multi_band+1, xdata*ydata)\n",
    "        W_hyper     : HS endmember matrix with ones (band+1, M)\n",
    "        H_hyper     : HS abundance matrix (M, xdata/w*ydata/w)\n",
    "        W_multi     : MS endmember matrix with ones (multi_band+1, M)\n",
    "        H_multi     : MS abundance matrix (M, xdata*ydata)\n",
    "        RMSE_h      : RMSE of HS unmixing\n",
    "        RMSE_m      : RMSE of MS unmixing\n",
    "    '''\n",
    "\n",
    "    MIN_MS_BANDS = 3\n",
    "\n",
    "    band = np.size(hyper,0)\n",
    "    multi_band = np.size(multi,0)\n",
    "    hx = int(xdata/w)\n",
    "    hy = int(ydata/w)\n",
    "    if verbose == 'on':\n",
    "        print('Initialize Wh by VCA')\n",
    "    W_hyper, indices = vca( hyper, M )\n",
    "\n",
    "    # Add manually defined endmembers\n",
    "    if np.isscalar(MEMs) == False:\n",
    "        W_hyper = np.hstack((W_hyper, MEMs))\n",
    "        M = W_hyper.shape[1]\n",
    "\n",
    "    # masking mode\n",
    "    if np.isscalar(mask):\n",
    "        masking = 0\n",
    "        mask = np.ones((hy,hx))\n",
    "    else:\n",
    "        masking = 1\n",
    "\n",
    "    # Initialize H_hyper: (M, N_h)\n",
    "    if masking == 0:\n",
    "        H_hyper = np.ones((M, hx*hy))/M\n",
    "    else:\n",
    "        H_hyper = np.ones((M, hx*hy))/M\n",
    "        H_hyper = H_hyper[:,mask.reshape(hx*hy)==1]\n",
    "\n",
    "    if init_mode == 1:\n",
    "        if verbose == 'on':\n",
    "            print('Initialize Hh by NLS')\n",
    "        # initialize H_hyper by nonnegative least squares\n",
    "        H_hyper = nls_su(hyper,W_hyper)\n",
    "\n",
    "    # Sum-to-one constraint\n",
    "    W_hyper = np.vstack((W_hyper, delta*np.ones((1,np.size(W_hyper, 1)))))\n",
    "    hyper = np.vstack((hyper, delta*np.ones((1,np.size(hyper, 1)))))\n",
    "\n",
    "    # NMF for Vh 1st\n",
    "    if verbose == 'on':\n",
    "        print ('NMF for Vh ( 1 )')\n",
    "    for i in range(I_in):\n",
    "        # Initialization of H_hyper\n",
    "        if i == 0:\n",
    "            cost0 = 0\n",
    "            for q in range(I_in*3):\n",
    "                # Update H_hyper\n",
    "                H_hyper_old = H_hyper\n",
    "                H_hyper_n = np.dot(W_hyper.transpose(), hyper)\n",
    "                H_hyper_d = np.dot(np.dot(W_hyper.transpose(), W_hyper), H_hyper)\n",
    "                H_hyper = (H_hyper*H_hyper_n)/H_hyper_d\n",
    "                cost = np.sum((hyper[0:band, :] - np.dot(W_hyper[0:band, :], H_hyper))**2)\n",
    "                if q > 1 and (cost0-cost)/cost < delta_h:\n",
    "                    if verbose == 'on':\n",
    "                        print('Initialization of H_hyper converged at the ', q, 'th iteration ')\n",
    "                    H_hyper = H_hyper_old\n",
    "                    break\n",
    "                cost0 = cost\n",
    "        else:\n",
    "            # Update W_hyper\n",
    "            W_hyper_old = W_hyper\n",
    "            W_hyper_n = np.dot(hyper[0:band, :], (H_hyper.transpose()))\n",
    "            W_hyper_d = np.dot(np.dot(W_hyper[0:band,:], H_hyper), H_hyper.transpose())\n",
    "            W_hyper[0:band, :] = (W_hyper[0:band, :]*W_hyper_n)/W_hyper_d\n",
    "            # Update H_hyper\n",
    "            H_hyper_old = H_hyper\n",
    "            H_hyper_n = np.dot(W_hyper.transpose(), hyper)\n",
    "            H_hyper_d = np.dot(np.dot(W_hyper.transpose(), W_hyper), H_hyper)\n",
    "            H_hyper = (H_hyper*H_hyper_n)/H_hyper_d\n",
    "            cost = np.sum((hyper[0:band, :] - np.dot(W_hyper[0:band, :], H_hyper))**2)\n",
    "            if (cost0-cost)/cost < delta_h:\n",
    "                if verbose == 'on':\n",
    "                    print('Optimization of HS unmixing converged at the ', i, 'th iteration ')\n",
    "                W_hyper = W_hyper_old\n",
    "                H_hyper = H_hyper_old\n",
    "                break\n",
    "            cost0 = cost\n",
    "\n",
    "    RMSE_h = (cost0/(hyper.shape[1]*band))**0.5\n",
    "    if verbose == 'on':\n",
    "        print('    RMSE(Vh) = ', RMSE_h)\n",
    "\n",
    "    # initialize W_multi: (multi_band, M)\n",
    "    W_multi = np.dot(srf, W_hyper[0:band,:])\n",
    "    W_multi = np.vstack((W_multi, delta*np.ones((1, M))))\n",
    "    multi = np.vstack((multi, delta*np.ones((1, multi.shape[1]))))\n",
    "\n",
    "    # initialize H_multi by interpolation\n",
    "    if masking == 0:\n",
    "        H_multi = np.ones((M, xdata*ydata))/M\n",
    "        for i in range(M):\n",
    "            tmp = zoom_bi(H_hyper[i,:].reshape(hx,hy).copy(),w)\n",
    "            H_multi[i,:] = tmp.reshape(1,xdata*ydata)\n",
    "        H_multi[np.nonzero(H_multi<0)] = 0\n",
    "    else:\n",
    "        mask2 = zoom_nn(mask,w)\n",
    "        H_multi = np.ones((M,multi.shape[1]))/M\n",
    "        for i in range(M):\n",
    "            tmp = np.zeros((hx,hy))\n",
    "            tmp[np.nonzero(mask>0)] = H_hyper[i,:].copy()\n",
    "            tmp = zoom_bi(tmp,w)\n",
    "            H_multi[i,:] = tmp[np.nonzero(mask2>0)].copy()\n",
    "        H_multi[np.nonzero(H_multi<0)] = 0\n",
    "\n",
    "    # NMF for Vm 1st\n",
    "    if verbose == 'on':\n",
    "        print('NMF for Vm ( 1 )')\n",
    "    for i in range(I_in):\n",
    "        if i == 0:\n",
    "            cost0 = 0\n",
    "            for q in range(I_in):\n",
    "                # Update H_multi\n",
    "                H_multi_old = H_multi\n",
    "                H_multi_n = np.dot(W_multi.transpose(), multi)\n",
    "                H_multi_d = np.dot(np.dot(W_multi.transpose(), W_multi), H_multi)\n",
    "                H_multi = (H_multi*H_multi_n)/H_multi_d\n",
    "                cost = np.sum((multi[0:multi_band, :] - np.dot(W_multi[0:multi_band, :], H_multi))**2)\n",
    "                if q > 1 and (cost0-cost)/cost < delta_m:\n",
    "                    if verbose == 'on':\n",
    "                        print('Initialization of H_multi converged at the ', q, 'th iteration ')\n",
    "                    H_multi = H_multi_old\n",
    "                    break\n",
    "                cost0 = cost\n",
    "        else:\n",
    "            # Update W_multi\n",
    "            W_multi_old = W_multi\n",
    "            if multi_band > MIN_MS_BANDS:\n",
    "                W_multi_n = np.dot(multi[0:multi_band, :], H_multi.transpose())\n",
    "                W_multi_d = np.dot(np.dot(W_multi[0:multi_band, :], H_multi), H_multi.transpose())\n",
    "                W_multi[0:multi_band, :] = (W_multi[0:multi_band, :]*W_multi_n)/W_multi_d\n",
    "            # Update H_hyper\n",
    "            H_multi_old = H_multi\n",
    "            H_multi_n = np.dot(W_multi.transpose(), multi)\n",
    "            H_multi_d = np.dot(np.dot(W_multi.transpose(), W_multi), H_multi)\n",
    "            H_multi = H_multi*H_multi_n/H_multi_d\n",
    "            cost = np.sum((multi[0:multi_band, :]-np.dot(W_multi[0:multi_band, :], H_multi))**2)\n",
    "            if (cost0-cost)/cost < delta_m:\n",
    "                if verbose == 'on':\n",
    "                    print('Optimization of MS unmixing converged at the ', i, 'th iteration ')\n",
    "                W_multi = W_multi_old\n",
    "                H_multi = H_multi_old\n",
    "                break\n",
    "            cost0=cost\n",
    "\n",
    "    RMSE_m = (cost0/((multi.shape[1])*multi_band))**0.5\n",
    "    if verbose == 'on':\n",
    "        print('    RMSE(Vm) = ', RMSE_m) # MSE(Mean Squared Error) in NMF of Vm\n",
    "\n",
    "    return hyper, multi, W_hyper, H_hyper, W_multi, H_multi, RMSE_h, RMSE_m\n",
    "\n",
    "def CNMF_ite(xdata,ydata,w,M,hyper,multi,W_hyper,H_hyper,W_multi,H_multi,I_in,delta_h,delta_m,I_out,i_out,srf,mask=0,verbose='off'):\n",
    "    '''\n",
    "    COUPLED NONNEGATIVE MATRIX FACTORIZATION (CNMF)\n",
    "\n",
    "    Copyright (c) 2016 Naoto Yokoya\n",
    "    Email: yokoya@sal.rcast.u-tokyo.ac.jp\n",
    "    Update: 2016/04/01\n",
    "\n",
    "    References:\n",
    "    [1] N. Yokoya, T. Yairi, and A. Iwasaki, \"Coupled nonnegative matrix\n",
    "        factorization unmixing for hyperspectral and multispectral data fusion,\"\n",
    "        IEEE Trans. Geosci. Remote Sens., vol. 50, no. 2, pp. 528-537, 2012.\n",
    "    [2] N. Yokoya, T. Yairi, and A. Iwasaki, \"Hyperspectral, multispectral,\n",
    "        and panchromatic data fusion based on non-negative matrix factorization,\"\n",
    "        Proc. WHISPERS, Lisbon, Portugal, Jun. 6-9, 2011.\n",
    "\n",
    "    This function is the iteration function of CNMF.\n",
    "\n",
    "    USAGE\n",
    "        W_hyper, H_hyper, W_multi1, H_multi1, W_multi2, H_multi2, RMSE_h, RMSE_m =\n",
    "        CNMF_ite(xdata,ydata,w,M,hyper,multi,W_hyper,H_hyper,W_multi,H_multi,ite_max,delta_h,delta_m,iter,srf,mask,verbose)\n",
    "\n",
    "    INPUT\n",
    "        xdata           : image height\n",
    "        ydata           : image width\n",
    "        w               : multiple difference of ground sampling distance (scalar)\n",
    "        M               : Number of endmembers\n",
    "        hyper           : Low-spatial-resolution HS image (band, xdata/w*ydata/w)\n",
    "        multi           : MS image (multi_band, xdata*ydata)\n",
    "        W_hyper         : HS endmember matrix with ones (band+1, M)\n",
    "        H_hyper         : HS abundance matrix (M, xdata/w*ydata/w)\n",
    "        W_multi         : MS endmember matrix with ones (multi_band+1, M)\n",
    "        H_multi         : MS abundance matrix (M, xdata*ydata)\n",
    "        delta           : Parameter of sum to one constraint\n",
    "        I_in            : Maximum number of inner iteration\n",
    "        delta_h         : Parameter for HS unmixing\n",
    "        delta_m         : Parameter for MS unmixing\n",
    "        I_out           : Maximum number of outer iteration\n",
    "        i_out           : Current number of outer iteration\n",
    "        srf             : Relative specctral response function\n",
    "        mask            : (optional) Binary mask for processing (xdata/w,ydata/w)\n",
    "\n",
    "    OUTPUT\n",
    "        W_hyper     : HS endmember matrix with ones (band+1, M)\n",
    "        H_hyper     : HS abundance matrix (M, xdata/w*ydata/w)\n",
    "        W_multi1    : MS endmember matrix with ones before MS unmixing (multi_band+1, M)\n",
    "        H_multi1    : MS abundance matrix before MS unmixing (M, xdata*ydata)\n",
    "        W_multi2    : MS endmember matrix with ones after MS unmixing (multi_band+1, M)\n",
    "        H_multi2    : MS abundance matrix after MS unmixing (M, xdata*ydata)\n",
    "        RMSE_h      : RMSE of HS unmixing\n",
    "        RMSE_m      : RMSE of MS unmixing\n",
    "    '''\n",
    "\n",
    "    MIN_MS_BANDS = 3\n",
    "\n",
    "    band = np.size(hyper,0)-1\n",
    "    multi_band = np.size(multi,0)-1\n",
    "    hx = int(xdata/w)\n",
    "    hy = int(ydata/w)\n",
    "\n",
    "    # masking mode\n",
    "    if np.isscalar(mask):\n",
    "        masking = 0\n",
    "        mask = np.ones((hy,hx))\n",
    "    else:\n",
    "        masking = 1\n",
    "\n",
    "    if verbose == 'on':\n",
    "        print('Iteration', i_out)\n",
    "\n",
    "    # Initialize H_hyper form H_multi\n",
    "    if masking == 0:\n",
    "        H_hyper = gaussian_down_sample(H_multi.transpose().reshape(xdata,ydata,M),w).reshape(hx*hy,M).transpose()\n",
    "    else:\n",
    "        mask2 = zoom_nn(mask,w)\n",
    "        for q in range(M):\n",
    "            tmp = np.zeros((xdata,ydata))\n",
    "            tmp[mask2>0] = H_multi[q,:].copy()\n",
    "            tmp = gaussian_down_sample(tmp.reshape(xdata,ydata,1),w).reshape(hx,hy)\n",
    "            H_hyper[q,:] = tmp[mask>0].copy().reshape(1,mask.sum())\n",
    "\n",
    "    # NMF for Vh\n",
    "    if verbose == 'on':\n",
    "        print('NMF for Vh (', i_out+2, ')')\n",
    "    for i in range(I_in):\n",
    "        if i == 0:\n",
    "            cost0 = 0\n",
    "            for q in range(I_in):\n",
    "                # Update W_hyper\n",
    "                W_hyper_old = W_hyper\n",
    "                W_hyper_n = np.dot(hyper[0:band, :], H_hyper.transpose())\n",
    "                W_hyper_d = np.dot(np.dot(W_hyper[0:band, :], H_hyper), H_hyper.transpose())\n",
    "                W_hyper[0:band, :] = (W_hyper[0:band, :]*W_hyper_n)/W_hyper_d\n",
    "                cost = np.sum((hyper[0:band, :] - np.dot(W_hyper[0:band, :], H_hyper))**2)\n",
    "                if q > 1 and (cost0-cost)/cost < delta_h:\n",
    "                    if verbose == 'on':\n",
    "                        print('Initialization of W_hyper converged at the ', q, 'th iteration ')\n",
    "                    W_hyper = W_hyper_old\n",
    "                    break\n",
    "                cost0 = cost\n",
    "        else:\n",
    "            # Update H_hyper\n",
    "            H_hyper_old = H_hyper\n",
    "            if multi_band > MIN_MS_BANDS:\n",
    "                H_hyper_n = np.dot(W_hyper.transpose(), hyper)\n",
    "                H_hyper_d = np.dot(np.dot(W_hyper.transpose(), W_hyper), H_hyper)\n",
    "                H_hyper = (H_hyper*H_hyper_n)/H_hyper_d\n",
    "            # Update W_hyper\n",
    "            W_hyper_old = W_hyper\n",
    "            W_hyper_n = np.dot(hyper[0:band, :], H_hyper.transpose())\n",
    "            W_hyper_d = np.dot(np.dot(W_hyper[0:band, :], H_hyper), H_hyper.transpose())\n",
    "            W_hyper[0:band, :] = (W_hyper[0:band, :]*W_hyper_n)/W_hyper_d\n",
    "            cost = np.sum((hyper[0:band, :] - np.dot(W_hyper[0:band, :], H_hyper))**2)\n",
    "            if (cost0-cost)/cost < delta_h:\n",
    "                if verbose == 'on':\n",
    "                    print('Optimization of HS unmixing converged at the ', i, 'th iteration ')\n",
    "                H_hyper = H_hyper_old\n",
    "                W_hyper = W_hyper_old\n",
    "                break\n",
    "            cost0 = cost\n",
    "\n",
    "    RMSE_h = (cost0/(hyper.shape[1]*band))**0.5\n",
    "    if verbose == 'on':\n",
    "        print('    RMSE(Vh) = ', RMSE_h)\n",
    "\n",
    "    W_multi1 = W_multi.copy()\n",
    "    H_multi1 = H_multi.copy()\n",
    "\n",
    "    # initialize W_multi: (multi_band, M)\n",
    "    W_multi[0:multi_band,:] = np.dot(srf, W_hyper[0:band,:])\n",
    "\n",
    "    if verbose == 'on':\n",
    "        print('NMF for Vm (', i_out+2, ')')\n",
    "    for i in range(I_in):\n",
    "        if i == 0:\n",
    "            cost0 = 0\n",
    "            for q in range(I_in):\n",
    "                # Update H_multi\n",
    "                H_multi_old = H_multi\n",
    "                H_multi_n = np.dot(W_multi.transpose(), multi)\n",
    "                H_multi_d = np.dot(np.dot(W_multi.transpose(), W_multi), H_multi)\n",
    "                H_multi = (H_multi*H_multi_n)/H_multi_d\n",
    "                cost = np.sum((multi[0:multi_band, :] - np.dot(W_multi[0:multi_band, :], H_multi))**2)\n",
    "                if q > 1 and (cost0-cost)/cost < delta_m:\n",
    "                    if verbose == 'on':\n",
    "                        print('Initialization of H_multi converged at the ', q, 'th iteration ')\n",
    "                    H_multi = H_multi_old\n",
    "                    break\n",
    "                cost0 = cost\n",
    "        else:\n",
    "            # Update W_multi\n",
    "            W_multi_old = W_multi\n",
    "            if multi_band > MIN_MS_BANDS:\n",
    "                W_multi_n = np.dot(multi[0:multi_band, :], H_multi.transpose())\n",
    "                W_multi_d = np.dot(np.dot(W_multi[0:multi_band, :], H_multi), H_multi.transpose())\n",
    "                W_multi[0:multi_band, :] = (W_multi[0:multi_band, :]*W_multi_n)/W_multi_d\n",
    "            # Update H_multi\n",
    "            H_multi_old = H_multi\n",
    "            H_multi_n = np.dot(W_multi.transpose(), multi)\n",
    "            H_multi_d = np.dot(np.dot(W_multi.transpose(), W_multi), H_multi)\n",
    "            H_multi = (H_multi*H_multi_n)/H_multi_d\n",
    "            cost = np.sum((multi[0:multi_band, :] - np.dot(W_multi[0:multi_band, :], H_multi))**2)\n",
    "            if (cost0-cost)/cost < delta_m:\n",
    "                if verbose == 'on':\n",
    "                    print('Optimization of MS unmixing converged at the ', i, 'th iteration ')\n",
    "                W_multi = W_multi_old\n",
    "                H_multi = H_multi_old\n",
    "                break\n",
    "            cost0 = cost\n",
    "\n",
    "    RMSE_m = (cost0/(multi.shape[1]*multi_band))**0.5\n",
    "    if verbose == 'on':\n",
    "        print('    RMSE(Vm) = ', RMSE_m)\n",
    "\n",
    "    W_multi2 = W_multi\n",
    "    H_multi2 = H_multi\n",
    "\n",
    "    return W_hyper, H_hyper, W_multi1, H_multi1, W_multi2, H_multi2, RMSE_h, RMSE_m\n",
    "\n",
    "def gaussian_filter2d(shape=(3,3),sigma=1):\n",
    "    '''\n",
    "    2D Gaussian filter\n",
    "\n",
    "    USAGE\n",
    "        h = gaussian_filter2d(shape,sigma)\n",
    "\n",
    "    INPUT\n",
    "        shape : window size (e.g., (3,3))\n",
    "        sigma : scalar\n",
    "\n",
    "    OUTPUT\n",
    "        h\n",
    "    '''\n",
    "    m,n = [(ss-1.)/2. for ss in shape]\n",
    "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
    "    h = np.exp( -(x**2 + y**2) / (2.*sigma**2) )\n",
    "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    return h\n",
    "\n",
    "def gaussian_down_sample(data,w,mask=0):\n",
    "    '''\n",
    "    This function downsamples HS image with a Gaussian point spread function.\n",
    "\n",
    "    USAGE\n",
    "          HSI = gaussian_down_sample(data,w,mask)\n",
    "\n",
    "    INPUT\n",
    "          data            : input HS image (xdata,ydata,band)\n",
    "          w               : difference of ground sampling distance (FWHM = w)\n",
    "          mask            : (optional) Binary mask for processing (xdata,ydata) (0: mask, 1: image)\n",
    "\n",
    "    OUTPUT\n",
    "          HSI             : downsampled HS image (xdata/w, ydata/w, band)\n",
    "    '''\n",
    "\n",
    "    # masking mode\n",
    "    if np.isscalar(mask):\n",
    "        masking = 0\n",
    "    else:\n",
    "        masking = 1\n",
    "\n",
    "    xdata = data.shape[0]\n",
    "    ydata = data.shape[1]\n",
    "    band = data.shape[2]\n",
    "    hx = int(np.floor(xdata/w))\n",
    "    hy = int(np.floor(ydata/w))\n",
    "    HSI = np.zeros((hx, hy, band))\n",
    "    sig = w/2.35482\n",
    "\n",
    "    if masking == 0: # without mask\n",
    "        if np.mod(w,2)==0:\n",
    "            H1 = gaussian_filter2d((w,w),sig).reshape(w,w,1)\n",
    "            H2 = gaussian_filter2d((w*2,w*2),sig).reshape(w*2,w*2,1)\n",
    "            for x in range(hx):\n",
    "                for y in range(hy):\n",
    "                    if x==0 or x==hx-1 or y==0 or y==hy-1:\n",
    "                        HSI[x,y,:] = (np.double( data[x*w:(x+1)*w,y*w:(y+1)*w,:] ) * np.tile(H1,(1,1,band))).sum(axis=0).sum(axis=0).reshape(1,1,band)\n",
    "                    else:\n",
    "                        HSI[x,y,:] = (np.double( data[x*w-w//2:(x+1)*w+w//2,y*w-w//2:(y+1)*w+w//2,:] ) * np.tile(H2,(1,1,band))).sum(axis=0).sum(axis=0).reshape(1,1,band)\n",
    "        else:\n",
    "            H1 = gaussian_filter2d((w,w),sig).reshape(w,w,1)\n",
    "            H2 = gaussian_filter2d((w*2-1,w*2-1),sig).reshape(w*2-1,w*2-1,1)\n",
    "            for x in range(hx):\n",
    "                for y in range(hy):\n",
    "                    if x==0 or x==hx-1 or y==0 or y==hy-1:\n",
    "                        HSI[x,y,:] = (np.double( data[x*w:(x+1)*w,y*w:(y+1)*w,:] ) * np.tile(H1,(1,1,band)) ).sum(axis=0).sum(axis=0).reshape(1,1,band)\n",
    "                    else:\n",
    "                        HSI[x,y,:] = (np.double( data[x*w-(w-1)//2:(x+1)*w+(w-1)//2,y*w-(w-1)//2:(y+1)*w+(w-1)//2,:] ) * np.tile(H2,(1,1,band))).sum(axis=0).sum(axis=0).reshape(1,1,band)\n",
    "    else: # with mask\n",
    "        if np.mod(w,2)==0:\n",
    "            H1 = gaussian_filter2d((w,w),sig).reshape(w,w,1)\n",
    "            H2 = gaussian_filter2d((w*2,w*2),sig).reshape(w*2,w*2,1)\n",
    "            for x in range(hx):\n",
    "                for y in range(hy):\n",
    "                    mask_tmp = mask[x*w:(x+1)*w,y*w:(y+1)*w]\n",
    "                    if mask_tmp.sum() == w**2:\n",
    "                        if x==0 or x==hx-1 or y==0 or y==hy-1:\n",
    "                            HSI[x,y,:] = (np.double( data[x*w:(x+1)*w,y*w:(y+1)*w,:] ) * np.tile(H1,(1,1,band))).sum(axis=0).sum(axis=0).reshape(1,1,band)\n",
    "                        else:\n",
    "                            HSI[x,y,:] = (np.double( data[x*w-w//2:(x+1)*w+w//2,y*w-w//2:(y+1)*w+w//2,:] ) * np.tile(H2,(1,1,band))).sum(axis=0).sum(axis=0).reshape(1,1,band)\n",
    "        else:\n",
    "            H1 = gaussian_filter2d((w,w),sig).reshape(w,w,1)\n",
    "            H2 = gaussian_filter2d((w*2-1,w*2-1),sig).reshape(w*2-1,w*2-1,1)\n",
    "            for x in range(hx):\n",
    "                for y in range(hy):\n",
    "                    mask_tmp = mask[x*w:(x+1)*w,y*w:(y+1)*w]\n",
    "                    if mask_tmp.sum() == w**2:\n",
    "                        if x==0 or x==hx-1 or y==0 or y==hy-1:\n",
    "                            HSI[x,y,:] = (np.double( data[x*w:(x+1)*w,y*w:(y+1)*w,:] ) * np.tile(H1,(1,1,band)) ).sum(axis=0).sum(axis=0).reshape(1,1,band)\n",
    "                        else:\n",
    "                            HSI[x,y,:] = (np.double( data[x*w-(w-1)//2:(x+1)*w+(w-1)//2,y*w-(w-1)//2:(y+1)*w+(w-1)//2,:] ) * np.tile(H2,(1,1,band))).sum(axis=0).sum(axis=0).reshape(1,1,band)\n",
    "\n",
    "    return HSI\n",
    "\n",
    "def zoom_nn(data,w):\n",
    "    '''\n",
    "    Zoom via nearest neighbor interpolation\n",
    "    '''\n",
    "    rows = data.shape[0]\n",
    "    cols = data.shape[1]\n",
    "    print(data.shape)\n",
    "    out = np.tile( np.tile(data.reshape(rows,cols,1),(1,1,w)).reshape(rows,cols*w,1) ,(1,1,w)).transpose(1,0,2).reshape(cols*w,rows*w).transpose()\n",
    "\n",
    "    return out\n",
    "\n",
    "def zoom_bi(data,w):\n",
    "    '''\n",
    "    Zoom via bilinear interpolation\n",
    "    '''\n",
    "    rows = data.shape[0]\n",
    "    cols = data.shape[1]\n",
    "    # index\n",
    "    r = np.tile(((2*np.r_[0:rows*w]+1)/(2*w)-0.5).reshape(rows*w,1),(1,cols*w))\n",
    "    c = np.tile((2*np.r_[0:cols*w]+1)/(2*w)-0.5,(rows*w,1))\n",
    "    r[r<0] = 0\n",
    "    r[r>rows-1] = rows-1\n",
    "    c[c<0] = 0\n",
    "    c[c>cols-1] = cols-1\n",
    "    w4 = (np.floor(r)+1-r)*(np.floor(c)+1-c)\n",
    "    w3 = (np.floor(r)+1-r)*(c-np.floor(c))\n",
    "    w2 = (r-np.floor(r))*(np.floor(c)+1-c)\n",
    "    w1 = (r-np.floor(r))*(c-np.floor(c))\n",
    "    data = np.hstack((np.vstack((data,np.zeros((1,cols)))),np.zeros((rows+1,1))))\n",
    "    out = w4*data[np.floor(r).astype(int),np.floor(c).astype(int)]+w3*data[np.floor(r).astype(int),np.floor(c).astype(int)+1]+w2*data[np.floor(r).astype(int)+1,np.floor(c).astype(int)]+w1*data[np.floor(r).astype(int)+1,np.floor(c).astype(int)+1]\n",
    "\n",
    "    return out\n",
    "\n",
    "def lsqnonneg(y,A):\n",
    "    '''\n",
    "    Nonnegative least squares via the active set method\n",
    "\n",
    "    This function solves the following optimization\n",
    "\n",
    "        min |y-Ax|^2\n",
    "        s.t. x>=0\n",
    "\n",
    "    USAGE\n",
    "        x = lsqnonneg(y,A)\n",
    "\n",
    "    INPUT\n",
    "        y  : observation (m,1)\n",
    "        A  : mixing matrix (m,n)\n",
    "\n",
    "    OUTPUT\n",
    "        x  : coefficients (n,1)\n",
    "    '''\n",
    "\n",
    "    t = 10*2.2204e-16*np.max(np.sum(np.abs(A),axis=0))*max([A.shape[0], A.shape[1]])\n",
    "\n",
    "    m = y.shape[0]\n",
    "    n = A.shape[1]\n",
    "\n",
    "    # initialize\n",
    "    x = np.zeros((n,1))\n",
    "    s = x.copy()\n",
    "    P = np.zeros((n,1))\n",
    "    R = np.ones((n,1))\n",
    "    w = np.dot(A.transpose() , (y - np.dot(A,x)))\n",
    "\n",
    "    # main loop\n",
    "    c = 0\n",
    "    while R.sum() > 0 and w.max() > t:\n",
    "        if c > 0:\n",
    "            j_pre = j\n",
    "        j = np.nonzero(w==w.max())\n",
    "        if c > 0:\n",
    "            if j == j_pre:\n",
    "                break\n",
    "        c = c+1\n",
    "\n",
    "        P[j[0]] = 1\n",
    "        R[j[0]] = 0\n",
    "        Ap = A[:,np.nonzero(P==1)[0]]\n",
    "        sp = np.dot( np.linalg.inv(np.dot(Ap.transpose(),Ap)) , np.dot(Ap.transpose(),y) )\n",
    "        s[np.nonzero(P==1)] = sp.reshape(1,len(sp))[0,:]\n",
    "        while s[np.nonzero(P==1)].min() <= 0:\n",
    "            if sum((s<=0)*((x-s)!=0)) != 0:\n",
    "                alpha = ( x[(s<=0)*((x-s)!=0)] / (x[(s<=0)*((x-s)!=0)]-s[(s<=0)*((x-s)!=0)]) ).min()\n",
    "                x = x + alpha*(s-x)\n",
    "                R[np.nonzero(x==0)] = 1\n",
    "                P[np.nonzero(x==0)] = 0\n",
    "                Ap = A[:,np.nonzero(P==1)[0]]\n",
    "                sp = np.dot( np.linalg.inv(np.dot(Ap.transpose(),Ap)) , np.dot(Ap.transpose(),y) )\n",
    "                s[np.nonzero(P==1)] = sp.reshape(1,len(sp))[0]\n",
    "                s[np.nonzero(R==1)] = 0\n",
    "            else:\n",
    "                break\n",
    "        x = s.copy()\n",
    "        w = np.dot(A.transpose() , (y - np.dot(A,x)))\n",
    "\n",
    "    return x\n",
    "\n",
    "def nls_su(Y,A):\n",
    "    '''\n",
    "    Nonnegative least squares for spectral unmixing\n",
    "\n",
    "    This function solves the following optimization\n",
    "\n",
    "        min |Y-AX|_F^2\n",
    "        s.t. X>=0\n",
    "\n",
    "    USAGE\n",
    "        X = nls_su(Y,A)\n",
    "\n",
    "    INPUT\n",
    "        Y  : observation (m,p)\n",
    "        A  : mixing matrix (m,n)\n",
    "\n",
    "    OUTPUT\n",
    "        X  : coefficients (n,p)\n",
    "    '''\n",
    "    n = A.shape[1]\n",
    "    p = Y.shape[1]\n",
    "    m = Y.shape[0]\n",
    "    X = np.zeros((p,n))\n",
    "    for i in range(p):\n",
    "        y = Y[:,i].reshape(m,1).copy()\n",
    "        x = lsqnonneg(y,A)\n",
    "        X[i,:] = x.transpose().copy()\n",
    "    print(n, p)\n",
    "\n",
    "    return X.transpose()\n",
    "\n",
    "def estR(HS,MS,mask=0):\n",
    "    '''\n",
    "    Estimation of relative spectral response functions (SRFs)\n",
    "    via the nonnegative least squares method\n",
    "\n",
    "    USAGE\n",
    "        R = estR(HS,MS,mask)\n",
    "\n",
    "    INPUT\n",
    "        HS  : Low-spatial-resolution HS image (rows2,cols2,bands2)\n",
    "        MS  : MS image (rows1,cols1,bands1)\n",
    "        mask: (optional) Binary mask for processing (rows2,cols2) (mainly\n",
    "              for real data)\n",
    "\n",
    "    OUTPUT\n",
    "        R   : Relative SRFs\n",
    "              without mask (bands1,bands2)\n",
    "              with mask    (bands1,bands2+1) (consider offset)\n",
    "    '''\n",
    "\n",
    "    rows1 = MS.shape[0]\n",
    "    cols1 = MS.shape[1]\n",
    "    bands1 = MS.shape[2]\n",
    "    rows2 = HS.shape[0]\n",
    "    cols2 = HS.shape[1]\n",
    "    bands2 = HS.shape[2]\n",
    "\n",
    "    # masking mode\n",
    "    if np.isscalar(mask):\n",
    "        masking = 0\n",
    "        mask = np.ones((rows2,cols2))\n",
    "    else:\n",
    "        masking = 1\n",
    "\n",
    "    HS = np.hstack((HS.reshape(rows2*cols2,bands2), mask.reshape(rows2*cols2,1) )).reshape(rows2,cols2,bands2+1)\n",
    "    bands2 = HS.shape[2]\n",
    "\n",
    "    R = np.zeros((bands1,bands2))\n",
    "\n",
    "    # downgrade spatial resolution\n",
    "    w = int(rows1/rows2)\n",
    "    mask2 = zoom_nn(mask,w)\n",
    "\n",
    "    Y = gaussian_down_sample(MS,w,mask2).reshape(rows2*cols2,bands1)\n",
    "\n",
    "    A = HS.reshape(rows2*cols2,bands2).copy()\n",
    "\n",
    "    if masking == 1:\n",
    "        Y = Y[mask.reshape(rows2*cols2)==1,:]\n",
    "        A = A[mask.reshape(rows2*cols2)==1,:]\n",
    "\n",
    "    # solve nonnegative least squares problems\n",
    "    for b in range(bands1):\n",
    "        y = Y[:,b].reshape(Y.shape[0],1).copy()\n",
    "        r = lsqnonneg(y,A)\n",
    "        R[b,:] = r.transpose().copy()\n",
    "\n",
    "    return R\n",
    "\n",
    "def vca(R,p):\n",
    "    '''\n",
    "    Vertex Component Analysis (VCA)\n",
    "\n",
    "    USAGE\n",
    "        U, indices = vca( R, p )\n",
    "\n",
    "    INPUT\n",
    "        R  : Hyperspectral data (bands,pixels)\n",
    "        p  : Number of endmembers\n",
    "\n",
    "    OUTPUT\n",
    "        U  : Matrix of endmembers (bands,p)\n",
    "        indices : Indices of endmembers in R\n",
    "\n",
    "    REFERENCE\n",
    "    J. M. P. Nascimento and J. M. B. Dias, \"Vertex component analysis: A\n",
    "    fast algorithm to unmix hyperspectral data,\" IEEE Transactions on\n",
    "    Geoscience and Remote Sensing, vol. 43, no. 4, pp. 898 - 910, Apr. 2005.\n",
    "    '''\n",
    "\n",
    "    N = R.shape[1] # pixels\n",
    "    L = R.shape[0] # bands\n",
    "\n",
    "    # Estimate SNR\n",
    "    r_m = R.mean(axis=1).reshape(L,1)\n",
    "    R_o = R - np.tile(r_m, (1, N))\n",
    "    U, S, V = np.linalg.svd(np.dot(R_o,R_o.T) / N)\n",
    "    Ud = U[:,:p] # computes the p-projection matrix\n",
    "    x_p = np.dot(Ud.T, R_o)\n",
    "    P_y = (R**2).sum() / N\n",
    "    P_x = (x_p**2).sum() / N + np.dot(r_m.T, r_m)\n",
    "    SNR = np.abs(10*np.log10( (P_x - (p/L)*P_y) / (P_y - P_x) ))\n",
    "\n",
    "    # Determine which projection to use.\n",
    "    SNRth = 15 + 10*np.log(p) + 8\n",
    "    #SNRth = 15 + 10*log(p) # threshold proposed in the original paper\n",
    "    if SNR > SNRth:\n",
    "        d = p\n",
    "        Ud, Sd, Vd = np.linalg.svd(np.dot(R,R.T)/N)\n",
    "        Ud = U[:,:d]\n",
    "        X = np.dot(Ud.T,R)\n",
    "        u = X.mean(axis=1).reshape(X.shape[0],1)\n",
    "        Y = X / np.tile( ( X * np.tile(u,(1, N)) ).sum(axis = 0) ,(d, 1) )\n",
    "    else:\n",
    "        d = p-1\n",
    "        r_m = (R.T).mean(axis=0).reshape((R.T).shape[1],1)\n",
    "        R_o = R - np.tile(r_m, (1, N))\n",
    "        Ud, Sd, Vd = np.linalg.svd(np.dot(R_o,R_o.T)/N)\n",
    "        Ud = U[:,:d]\n",
    "        X = np.dot(Ud.T, R_o)\n",
    "        c = np.sqrt((X**2).sum(axis = 0).max())\n",
    "        c = np.tile(c, (1, N))\n",
    "        Y = np.vstack( (X, c) )\n",
    "\n",
    "    e_u = np.zeros((p, 1))\n",
    "    e_u[p-1,0] = 1\n",
    "    A = np.zeros((p, p))\n",
    "    A[:,0] = e_u[:,0]\n",
    "\n",
    "    I = np.eye(p)\n",
    "    k = np.zeros((N, 1))\n",
    "\n",
    "    indices = []\n",
    "    for i in range(p):\n",
    "        w = np.random.rand(p,1)\n",
    "        f = np.dot((I-np.dot(A,np.linalg.pinv(A))), w)\n",
    "        f = f / np.linalg.norm(f)\n",
    "        v = np.dot(f.T,Y)\n",
    "        k = np.abs(v).argmax()\n",
    "        A[:,i] = Y[:,k]\n",
    "        indices.append(k)\n",
    "\n",
    "    if SNR > SNRth:\n",
    "        U = np.dot(Ud,X[:,indices])\n",
    "    else:\n",
    "        U = np.dot(Ud,X[:,indices]) + np.tile(r_m, (1, p))\n",
    "\n",
    "    return U, indices\n",
    "\n",
    "def vd(data,alpha=10**(-3)):\n",
    "    '''\n",
    "    Virtual dimensionality\n",
    "\n",
    "    USAGE\n",
    "        out = vd(data,alpha)\n",
    "\n",
    "    INPUT\n",
    "        data : HSI data (bands,pizels)\n",
    "        alpha: False alarm rate\n",
    "\n",
    "    OUTPUT\n",
    "        out  : Number of spectrally distinct signal sources in data\n",
    "\n",
    "    REFERENCE\n",
    "    J. Harsanyi, W. Farrand, and C.-I Chang, \"Determining the number and\n",
    "    identity of spectral endmembers: An integrated approach using\n",
    "    Neyman-Pearson eigenthresholding and iterative constrained RMS error\n",
    "    minimization,\" in Proc. 9th Thematic Conf. Geologic Remote Sensing,\n",
    "    Feb. 1993.\n",
    "    Chang, C.-I. and Du, Q., \"Estimation of number of spectrally distinct\n",
    "    signal sources in hyperspectral imagery,\" IEEE Transactions on Geoscience\n",
    "    and Remote Sensing, vol. 42, pp. 608-619, 2004.\n",
    "    '''\n",
    "    data = np.double(data)\n",
    "    N = data.shape[1] # pixels\n",
    "    L = data.shape[0] # bands\n",
    "\n",
    "    R = np.dot(data, data.T)/N\n",
    "    K = np.cov(data)\n",
    "\n",
    "    D_r, V_r = np.linalg.eig(R)\n",
    "    D_k, V_k = np.linalg.eig(K)\n",
    "\n",
    "    e_r = np.sort(D_r)[::-1]\n",
    "    e_k = np.sort(D_k)[::-1]\n",
    "\n",
    "    diff = e_r - e_k\n",
    "    variance = (2*(e_r**2+e_k**2)/N)**0.5\n",
    "\n",
    "    tau = -ppf(alpha,np.zeros(L),variance)\n",
    "\n",
    "    out = sum(diff > tau)\n",
    "\n",
    "    return out\n",
    "\n",
    "def PSNR(ref,tar,mask=0):\n",
    "    '''\n",
    "    Peak signal to noise ratio (PSNR)\n",
    "\n",
    "    USAGE\n",
    "        psnr_all, psnr_mean = PSNR(ref,tar)\n",
    "\n",
    "    INPUT\n",
    "        ref : reference HS data (rows,cols,bands)\n",
    "        tar : target HS data (rows,cols,bands)\n",
    "        mask: (optional) Binary mask for processing  (rows,cols) (0: mask, 1: image)\n",
    "\n",
    "    OUTPUT\n",
    "        psnr_all  : PSNR (bands)\n",
    "        psnr_mean : average PSNR (scalar)\n",
    "    '''\n",
    "    rows = ref.shape[0]\n",
    "    cols = ref.shape[1]\n",
    "    bands = ref.shape[2]\n",
    "\n",
    "    # masking mode\n",
    "    if np.isscalar(mask):\n",
    "        mask = np.ones((rows,cols))\n",
    "\n",
    "    ref = ref.reshape(rows*cols,bands)\n",
    "    tar = tar.reshape(rows*cols,bands)\n",
    "    mask = mask.reshape(rows*cols)\n",
    "    msr = ((ref[mask==1,:]-tar[mask==1,:])**2).mean(axis=0)\n",
    "    max2 = ref.max(axis=0)**2\n",
    "\n",
    "    psnr_all = 10*np.log10(max2/msr)\n",
    "    psnr_mean = psnr_all.mean()\n",
    "\n",
    "    return psnr_all, psnr_mean\n",
    "\n",
    "def SAM(ref,tar,mask=0):\n",
    "    '''\n",
    "    Spectral angle mapper (SAM)\n",
    "\n",
    "    USAGE\n",
    "        sam_mean, map = SAM(ref,tar)\n",
    "\n",
    "    INPUT\n",
    "        ref : reference HS data (rows,cols,bands)\n",
    "        tar : target HS data (rows,cols,bands)\n",
    "        mask: (optional) Binary mask for processing  (rows,cols) (0: mask, 1: image)\n",
    "\n",
    "    OUTPUT\n",
    "        sam_mean : average value of SAM (scalar in degree)\n",
    "        map      : 2-D map (in degree)\n",
    "    '''\n",
    "    rows = tar.shape[0]\n",
    "    cols = tar.shape[1]\n",
    "    bands = tar.shape[2]\n",
    "\n",
    "    # masking mode\n",
    "    if np.isscalar(mask):\n",
    "        masking = 0\n",
    "        mask = np.ones(rows*cols)\n",
    "    else:\n",
    "        masking = 1\n",
    "        mask = mask.reshape(rows*cols)\n",
    "\n",
    "    prod_scal = (ref*tar).sum(axis=2)\n",
    "    norm_orig = (ref*ref).sum(axis=2)\n",
    "    norm_fusa = (tar*tar).sum(axis=2)\n",
    "    prod_norm = np.sqrt(norm_orig*norm_fusa)\n",
    "    prod_map = prod_norm\n",
    "    prod_map[prod_map==0] = 2.2204e-16\n",
    "    map = np.real(np.arccos(prod_scal/prod_map))*180/np.pi\n",
    "    prod_scal = prod_scal.reshape(rows*cols)\n",
    "    prod_norm = prod_norm.reshape(rows*cols)\n",
    "    sam_mean = np.real(np.arccos(prod_scal[(prod_norm!=0)*(mask==1)]/prod_norm[(prod_norm!=0)*(mask==1)]).sum()/((prod_norm!=0)*(mask==1)).sum())*180/np.pi\n",
    "\n",
    "    return sam_mean, map\n",
    "\n",
    "def ppf(p,mu=0,sigma=1):\n",
    "    '''\n",
    "    Percent point function (inverse of cdf)\n",
    "    for the normal distribution at p\n",
    "\n",
    "    USAGE\n",
    "        out = ppf(p,mu,sigma)\n",
    "\n",
    "    INPUT\n",
    "        p     : lower tail probability\n",
    "        mu    : mean (n)\n",
    "        sigma : standard deviation (n)\n",
    "\n",
    "    OUTPUT\n",
    "        out   : quantile corresponding to the lower tail probability p (n)\n",
    "    '''\n",
    "    n = mu.shape[0] # number of elements\n",
    "    out = np.zeros((n))\n",
    "    for i in range(n):\n",
    "        #print sigma[i]\n",
    "        out[i] = 2**0.5*sigma[i]*erfinv(2*p-1)+mu[i]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287224ef",
   "metadata": {},
   "source": [
    "# GFPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbfb8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_interp23\n",
    "from sklearn.decomposition import PCA as princomp\n",
    "from cv2.ximgproc import guidedFilter\n",
    "\n",
    "def GFPCA(pan, hs):\n",
    "\n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    p = princomp(n_components=C)\n",
    "    pca_hs = p.fit_transform(np.reshape(hs, (m*n, C)))\n",
    "    \n",
    "    pca_hs = np.reshape(pca_hs, (m, n, C))\n",
    "    \n",
    "    pca_hs = upsample_interp23(pca_hs, ratio)\n",
    "    \n",
    "    gp_hs = []\n",
    "    for i in range(C):\n",
    "        temp = guidedFilter(np.float32(pan), np.float32(np.expand_dims(pca_hs[:, :, i], -1)), 8, eps = 0.001**2)\n",
    "        temp = np.expand_dims(temp ,axis=-1)\n",
    "        gp_hs.append(temp)\n",
    "        \n",
    "    gp_hs = np.concatenate(gp_hs, axis=-1)\n",
    "    \n",
    "    I_GFPCA = p.inverse_transform(gp_hs)\n",
    "    \n",
    "    #adjustment\n",
    "    I_GFPCA[I_GFPCA<0]=0\n",
    "    I_GFPCA[I_GFPCA>1]=1\n",
    "    \n",
    "    return np.uint8(I_GFPCA*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bc633",
   "metadata": {},
   "source": [
    "# GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fde20d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_interp23\n",
    "\n",
    "def GS(pan, hs):\n",
    "\n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    #upsample\n",
    "    u_hs = upsample_interp23(hs, ratio)\n",
    "    \n",
    "    #remove means from u_hs\n",
    "    means = np.mean(u_hs, axis=(0, 1))\n",
    "    image_lr = u_hs-means\n",
    "    \n",
    "    #sintetic intensity\n",
    "    I = np.mean(u_hs, axis=2, keepdims=True)\n",
    "    I0 = I-np.mean(I)\n",
    "    \n",
    "    image_hr = (pan-np.mean(pan))*(np.std(I0, ddof=1)/np.std(pan, ddof=1))+np.mean(I0)\n",
    "    \n",
    "    #computing coefficients\n",
    "    g = []\n",
    "    g.append(1)\n",
    "    \n",
    "    for i in range(C):\n",
    "        temp_h = image_lr[:, :, i]\n",
    "        c = np.cov(np.reshape(I0, (-1,)), np.reshape(temp_h, (-1,)), ddof=1)\n",
    "        g.append(c[0,1]/np.var(I0))\n",
    "    g = np.array(g)\n",
    "    \n",
    "    #detail extraction\n",
    "    delta = image_hr-I0\n",
    "    deltam = np.tile(delta, (1, 1, C+1))\n",
    "    \n",
    "    #fusion\n",
    "    V = np.concatenate((I0, image_lr), axis=-1)\n",
    "    \n",
    "    g = np.expand_dims(g, 0)\n",
    "    g = np.expand_dims(g, 0)\n",
    "    \n",
    "    g = np.tile(g, (M, N, 1))\n",
    "    \n",
    "    V_hat = V+ g*deltam\n",
    "    \n",
    "    I_GS = V_hat[:, :, 1:]\n",
    "    \n",
    "    I_GS = I_GS - np.mean(I_GS, axis=(0, 1))+means\n",
    "    \n",
    "    #adjustment\n",
    "    I_GS[I_GS<0]=0\n",
    "    I_GS[I_GS>1]=1\n",
    "    \n",
    "    return np.uint8(I_GS*255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da0821",
   "metadata": {},
   "source": [
    "# GSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa308bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_interp23\n",
    "import cv2\n",
    "\n",
    "def estimation_alpha(pan, hs, mode='global'):\n",
    "    if mode == 'global':\n",
    "        IHC = np.reshape(pan, (-1, 1))\n",
    "        ILRC = np.reshape(hs, (hs.shape[0]*hs.shape[1], hs.shape[2]))\n",
    "        \n",
    "        alpha = np.linalg.lstsq(ILRC, IHC)[0]\n",
    "        \n",
    "    elif mode == 'local':\n",
    "        patch_size = 32\n",
    "        all_alpha = []\n",
    "        print(pan.shape)\n",
    "        for i in range(0, hs.shape[0]-patch_size, patch_size):\n",
    "            for j in range(0, hs.shape[1]-patch_size, patch_size):\n",
    "                patch_pan = pan[i:i+patch_size, j:j+patch_size, :]\n",
    "                patch_hs = hs[i:i+patch_size, j:j+patch_size, :]\n",
    "                \n",
    "                IHC = np.reshape(patch_pan, (-1, 1))\n",
    "                ILRC = np.reshape(patch_hs, (-1, hs.shape[2]))\n",
    "                \n",
    "                local_alpha = np.linalg.lstsq(ILRC, IHC)[0]\n",
    "                all_alpha.append(local_alpha)\n",
    "                \n",
    "        all_alpha = np.array(all_alpha)\n",
    "        \n",
    "        alpha = np.mean(all_alpha, axis=0, keepdims=False)\n",
    "        \n",
    "    return alpha\n",
    "\n",
    "def GSA(pan, hs):\n",
    "    \n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    #upsample\n",
    "    u_hs = upsample_interp23(hs, ratio)\n",
    "    \n",
    "    #remove means from u_hs\n",
    "    means = np.mean(u_hs, axis=(0, 1))\n",
    "    image_lr = u_hs-means\n",
    "    \n",
    "    #remove means from hs\n",
    "    image_lr_lp = hs-np.mean(hs, axis=(0,1))\n",
    "    \n",
    "    #sintetic intensity\n",
    "    image_hr = pan-np.mean(pan)\n",
    "    image_hr0 = cv2.resize(image_hr, (n, m), cv2.INTER_CUBIC)\n",
    "    image_hr0 = np.expand_dims(image_hr0, -1)\n",
    "    \n",
    "    alpha = estimation_alpha(image_hr0, np.concatenate((image_lr_lp, np.ones((m, n, 1))), axis=-1), mode='global')\n",
    "    \n",
    "    I = np.dot(np.concatenate((image_lr, np.ones((M, N, 1))), axis=-1), alpha)\n",
    "    \n",
    "    I0 = I-np.mean(I)\n",
    "    \n",
    "    #computing coefficients\n",
    "    g = []\n",
    "    g.append(1)\n",
    "    \n",
    "    for i in range(C):\n",
    "        temp_h = image_lr[:, :, i]\n",
    "        c = np.cov(np.reshape(I0, (-1,)), np.reshape(temp_h, (-1,)), ddof=1)\n",
    "        g.append(c[0,1]/np.var(I0))\n",
    "    g = np.array(g)\n",
    "    \n",
    "    #detail extraction\n",
    "    delta = image_hr-I0\n",
    "    deltam = np.tile(delta, (1, 1, C+1))\n",
    "    \n",
    "    #fusion\n",
    "    V = np.concatenate((I0, image_lr), axis=-1)\n",
    "    \n",
    "    g = np.expand_dims(g, 0)\n",
    "    g = np.expand_dims(g, 0)\n",
    "    \n",
    "    g = np.tile(g, (M, N, 1))\n",
    "    \n",
    "    V_hat = V + g*deltam\n",
    "    \n",
    "    I_GSA = V_hat[:, :, 1:]\n",
    "    \n",
    "    I_GSA = I_GSA - np.mean(I_GSA, axis=(0, 1)) + means\n",
    "    \n",
    "    #adjustment\n",
    "    I_GSA[I_GSA<0]=0\n",
    "    I_GSA[I_GSA>1]=1\n",
    "    \n",
    "    return np.uint8(I_GSA*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad302fa",
   "metadata": {},
   "source": [
    "# IHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08746a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_interp23\n",
    "\n",
    "def IHS(pan, hs):\n",
    "\n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    #upsample\n",
    "    u_hs = upsample_interp23(hs, ratio)\n",
    "    \n",
    "    I = np.mean(u_hs, axis=-1, keepdims=True)\n",
    "    \n",
    "    P = (pan - np.mean(pan))*np.std(I, ddof=1)/np.std(pan, ddof=1)+np.mean(I)\n",
    "    \n",
    "    I_IHS = u_hs + np.tile(P-I, (1, 1, C))\n",
    "    \n",
    "    #adjustment\n",
    "    I_IHS[I_IHS<0]=0\n",
    "    I_IHS[I_IHS>1]=1\n",
    "    \n",
    "    return np.uint8(I_IHS*255)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b618b7",
   "metadata": {},
   "source": [
    "# MTF GLP HPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43fc8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_interp23\n",
    "import cv2\n",
    "from scipy import signal\n",
    "\n",
    "def gaussian2d(N, std):\n",
    "    \n",
    "    t=np.arange(-(N-1)/2,(N+2)/2)\n",
    "    t1,t2=np.meshgrid(t,t)\n",
    "    std=np.double(std)\n",
    "    w = np.exp(-0.5*(t1/std)**2)*np.exp(-0.5*(t2/std)**2) \n",
    "    return w\n",
    "    \n",
    "def kaiser2d(N, beta):\n",
    "    \n",
    "    t=np.arange(-(N-1)/2,(N+1)/2)/np.double(N-1)\n",
    "    t1,t2=np.meshgrid(t,t)\n",
    "    t12=np.sqrt(t1*t1+t2*t2)\n",
    "    w1=np.kaiser(N,beta)\n",
    "    w=np.interp(t12,t,w1)\n",
    "    w[t12>t[-1]]=0\n",
    "    w[t12<t[0]]=0\n",
    "    \n",
    "    return w\n",
    "\n",
    "def fir_filter_wind(Hd,w):\n",
    "    \"\"\"\n",
    "\tcompute fir filter with window method\n",
    "\tHd: \tdesired freqeuncy response (2D)\n",
    "\tw: \t\twindow (2D)\n",
    "\t\"\"\"\n",
    "\t\n",
    "    hd=np.rot90(np.fft.fftshift(np.rot90(Hd,2)),2)\n",
    "    h=np.fft.fftshift(np.fft.ifft2(hd))\n",
    "    h=np.rot90(h,2)\n",
    "    h=h*w\n",
    "    h=h/np.sum(h)\n",
    "    \n",
    "    return h\n",
    "\n",
    "def MTF_GLP_HPM(pan, hs, sensor='gaussian'):\n",
    "    \n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    #upsample\n",
    "    u_hs = upsample_interp23(hs, ratio)\n",
    "    \n",
    "    #equalization\n",
    "    image_hr = np.tile(pan, (1, 1, C))\n",
    "    \n",
    "    image_hr = (image_hr - np.mean(image_hr, axis=(0,1)))*(np.std(u_hs, axis=(0, 1), ddof=1)/np.std(image_hr, axis=(0, 1), ddof=1))+np.mean(u_hs, axis=(0,1))\n",
    "    \n",
    "    pan_lp = np.zeros_like(u_hs)\n",
    "    N =31\n",
    "    fcut = 1/ratio\n",
    "    match = 0\n",
    "    \n",
    "    if sensor == 'gaussian':\n",
    "        sig = (1/(2*(2.772587)/ratio**2))**0.5\n",
    "        kernel = np.multiply(cv2.getGaussianKernel(9, sig), cv2.getGaussianKernel(9,sig).T)\n",
    "        \n",
    "        t=[]\n",
    "        for i in range(C):\n",
    "            temp = signal.convolve2d(image_hr[:, :, i], kernel, mode='same', boundary = 'wrap')\n",
    "            temp = temp[0::ratio, 0::ratio]\n",
    "            temp = np.expand_dims(temp, -1)\n",
    "            t.append(temp)\n",
    "        \n",
    "        t = np.concatenate(t, axis=-1)\n",
    "        pan_lp = upsample_interp23(t, ratio)\n",
    "    \n",
    "    elif sensor == None:\n",
    "        match=1\n",
    "        GNyq = 0.3*np.ones((C,))\n",
    "    elif sensor=='QB':\n",
    "        match=1\n",
    "        GNyq = np.asarray([0.34, 0.32, 0.30, 0.22],dtype='float32')    # Band Order: B,G,R,NIR\n",
    "    elif sensor=='IKONOS':\n",
    "        match=1           #MTF usage\n",
    "        GNyq = np.asarray([0.26,0.28,0.29,0.28],dtype='float32')    # Band Order: B,G,R,NIR\n",
    "    elif sensor=='GeoEye1':\n",
    "        match=1             # MTF usage\n",
    "        GNyq = np.asarray([0.23,0.23,0.23,0.23],dtype='float32')    # Band Order: B,G,R,NIR   \n",
    "    elif sensor=='WV2':\n",
    "        match=1            # MTF usage\n",
    "        GNyq = [0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.27]\n",
    "    elif sensor=='WV3':\n",
    "        match=1             #MTF usage\n",
    "        GNyq = 0.29 * np.ones(8)\n",
    "    \n",
    "    if match==1:\n",
    "        t = []\n",
    "        for i in range(C):\n",
    "            alpha = np.sqrt(N*(fcut/2)**2/(-2*np.log(GNyq)))\n",
    "            H = np.multiply(cv2.getGaussianKernel(N, alpha[i]), cv2.getGaussianKernel(N, alpha[i]).T)\n",
    "            HD = H/np.max(H)\n",
    "            \n",
    "            h = fir_filter_wind(HD, kaiser2d(N, 0.5))\n",
    "            \n",
    "            temp = signal.convolve2d(image_hr[:, :, i], np.real(h), mode='same', boundary = 'wrap')\n",
    "            temp = temp[0::ratio, 0::ratio]\n",
    "            temp = np.expand_dims(temp, -1)\n",
    "            t.append(temp)\n",
    "        \n",
    "        t = np.concatenate(t, axis=-1)\n",
    "        pan_lp = upsample_interp23(t, ratio)\n",
    "        \n",
    "    I_MTF_GLP_HPM = u_hs*(image_hr/(pan_lp+1e-8))      \n",
    "    \n",
    "    #adjustment\n",
    "    I_MTF_GLP_HPM[I_MTF_GLP_HPM<0]=0\n",
    "    I_MTF_GLP_HPM[I_MTF_GLP_HPM>1]=1\n",
    "    \n",
    "    return np.uint8(I_MTF_GLP_HPM*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbb49d",
   "metadata": {},
   "source": [
    "# MTF GLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "665625f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_interp23\n",
    "import cv2\n",
    "from scipy import signal\n",
    "\n",
    "def gaussian2d(N, std):\n",
    "    \n",
    "    t=np.arange(-(N-1)/2,(N+2)/2)\n",
    "    t1,t2=np.meshgrid(t,t)\n",
    "    std=np.double(std)\n",
    "    w = np.exp(-0.5*(t1/std)**2)*np.exp(-0.5*(t2/std)**2) \n",
    "    return w\n",
    "    \n",
    "def kaiser2d(N, beta):\n",
    "    \n",
    "    t=np.arange(-(N-1)/2,(N+1)/2)/np.double(N-1)\n",
    "    t1,t2=np.meshgrid(t,t)\n",
    "    t12=np.sqrt(t1*t1+t2*t2)\n",
    "    w1=np.kaiser(N,beta)\n",
    "    w=np.interp(t12,t,w1)\n",
    "    w[t12>t[-1]]=0\n",
    "    w[t12<t[0]]=0\n",
    "    \n",
    "    return w\n",
    "\n",
    "def fir_filter_wind(Hd,w):\n",
    "    \"\"\"\n",
    "\tcompute fir filter with window method\n",
    "\tHd: \tdesired freqeuncy response (2D)\n",
    "\tw: \t\twindow (2D)\n",
    "\t\"\"\"\n",
    "\t\n",
    "    hd=np.rot90(np.fft.fftshift(np.rot90(Hd,2)),2)\n",
    "    h=np.fft.fftshift(np.fft.ifft2(hd))\n",
    "    h=np.rot90(h,2)\n",
    "    h=h*w\n",
    "    h=h/np.sum(h)\n",
    "    \n",
    "    return h\n",
    "\n",
    "def MTF_GLP(pan, hs, sensor='gaussian'):\n",
    "    \n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    #upsample\n",
    "    u_hs = upsample_interp23(hs, ratio)\n",
    "    \n",
    "    #equalization\n",
    "    image_hr = np.tile(pan, (1, 1, C))\n",
    "    \n",
    "    image_hr = (image_hr - np.mean(image_hr, axis=(0,1)))*(np.std(u_hs, axis=(0, 1), ddof=1)/np.std(image_hr, axis=(0, 1), ddof=1))+np.mean(u_hs, axis=(0,1))\n",
    "    \n",
    "#    print(image_hr.shape)\n",
    "    \n",
    "    pan_lp = np.zeros_like(u_hs)\n",
    "    N =31\n",
    "    fcut = 1/ratio\n",
    "    match = 0\n",
    "    \n",
    "    if sensor == 'gaussian':\n",
    "        sig = (1/(2*(2.772587)/ratio**2))**0.5\n",
    "        kernel = np.multiply(cv2.getGaussianKernel(9, sig), cv2.getGaussianKernel(9,sig).T)\n",
    "        \n",
    "        t=[]\n",
    "        for i in range(C):\n",
    "            temp = signal.convolve2d(image_hr[:, :, i], kernel, mode='same', boundary = 'wrap')\n",
    "            temp = temp[0::ratio, 0::ratio]\n",
    "            temp = np.expand_dims(temp, -1)\n",
    "            t.append(temp)\n",
    "        \n",
    "        t = np.concatenate(t, axis=-1)\n",
    "        pan_lp = upsample_interp23(t, ratio)\n",
    "    \n",
    "    elif sensor == None:\n",
    "        match=1\n",
    "        GNyq = 0.3*np.ones((C,))\n",
    "    elif sensor=='QB':\n",
    "        match=1\n",
    "        GNyq = np.asarray([0.34, 0.32, 0.30, 0.22],dtype='float32')    # Band Order: B,G,R,NIR\n",
    "    elif sensor=='IKONOS':\n",
    "        match=1           #MTF usage\n",
    "        GNyq = np.asarray([0.26,0.28,0.29,0.28],dtype='float32')    # Band Order: B,G,R,NIR\n",
    "    elif sensor=='GeoEye1':\n",
    "        match=1             # MTF usage\n",
    "        GNyq = np.asarray([0.23,0.23,0.23,0.23],dtype='float32')    # Band Order: B,G,R,NIR   \n",
    "    elif sensor=='WV2':\n",
    "        match=1            # MTF usage\n",
    "        GNyq = [0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.27]\n",
    "    elif sensor=='WV3':\n",
    "        match=1             #MTF usage\n",
    "        GNyq = 0.29 * np.ones(8)\n",
    "    \n",
    "    if match==1:\n",
    "        t = []\n",
    "        for i in range(C):\n",
    "            alpha = np.sqrt(N*(fcut/2)**2/(-2*np.log(GNyq)))\n",
    "            H = np.multiply(cv2.getGaussianKernel(N, alpha[i]), cv2.getGaussianKernel(N, alpha[i]).T)\n",
    "            HD = H/np.max(H)\n",
    "            \n",
    "            h = fir_filter_wind(HD, kaiser2d(N, 0.5))\n",
    "            \n",
    "            temp = signal.convolve2d(image_hr[:, :, i], np.real(h), mode='same', boundary = 'wrap')\n",
    "            temp = temp[0::ratio, 0::ratio]\n",
    "            temp = np.expand_dims(temp, -1)\n",
    "            t.append(temp)\n",
    "        \n",
    "        t = np.concatenate(t, axis=-1)\n",
    "        pan_lp = upsample_interp23(t, ratio)\n",
    "        \n",
    "    I_MTF_GLP = u_hs + image_hr - pan_lp        \n",
    "    \n",
    "    #adjustment\n",
    "    I_MTF_GLP[I_MTF_GLP<0]=0\n",
    "    I_MTF_GLP[I_MTF_GLP>1]=1\n",
    "    \n",
    "    return np.uint8(I_MTF_GLP*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8f04d",
   "metadata": {},
   "source": [
    "# PanNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "462aa618",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Concatenate, Conv2D, Input, Layer, Add, Activation, BatchNormalization\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningRateScheduler, ModelCheckpoint\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Concatenate, Conv2D, Input, Layer, Add, Activation, BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras import backend as K\n",
    "import os\n",
    "import random\n",
    "#from utils import downgrade_images\n",
    "import gc\n",
    "\n",
    "def psnr(y_true, y_pred):\n",
    "    \"\"\"Peak signal-to-noise ratio averaged over samples and channels.\"\"\"\n",
    "    mse = K.mean(K.square(y_true*255 - y_pred*255), axis=(-3, -2, -1))\n",
    "    return K.mean(20 * K.log(255 / K.sqrt(mse)) / np.log(10))\n",
    "\n",
    "class hp_filter(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(hp_filter, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        c = inputs.get_shape().as_list()[-1]\n",
    "        \n",
    "        kernel = np.ones((5,5))/25.0\n",
    "        kernel = K.constant(kernel)\n",
    "        \n",
    "        kernel = K.expand_dims(kernel, -1)\n",
    "        kernel = K.expand_dims(kernel, -1)\n",
    "        \n",
    "        kernel = K.tile(kernel, (1, 1, c, 1))\n",
    "        \n",
    "        outs = K.depthwise_conv2d(inputs, kernel, strides=(1, 1), padding='same')\n",
    "        \n",
    "        outs = inputs - outs\n",
    "        \n",
    "        self.outs_size = outs.get_shape().as_list()\n",
    "        \n",
    "        return outs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple(self.outs_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(hp_filter, self).get_config()\n",
    "        return config\n",
    "\n",
    "class resize(Layer):\n",
    "    def __init__(self, target_size,\n",
    "                 **kwargs):\n",
    "        self.target_size = (target_size[0], target_size[1])\n",
    "        super(resize, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        temp = tf.image.resize_bicubic(inputs, self.target_size)\n",
    "        return temp\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.target_size[0], self.target_size[1], input_shape[3])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(resize, self).get_config()\n",
    "        return config\n",
    "    \n",
    "def conv_block(inputs, block_name='1'):\n",
    "    \n",
    "    conv1 = Conv2D(32, (3, 3), strides=(1, 1), padding='same', name=block_name+'_1')(inputs)\n",
    "#    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv2 = Conv2D(32, (3, 3), strides=(1, 1), padding='same', name=block_name+'_2')(conv1)\n",
    "#    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "\n",
    "    outputs = Add()([inputs, conv2])\n",
    "    return outputs\n",
    "\n",
    "def pannet(lrhs_size=(16, 16, 3), hrms_size = (64, 64, 1)):\n",
    "    \n",
    "    lrhs_inputs = Input(lrhs_size)\n",
    "    hrms_inputs = Input(hrms_size)\n",
    "    \n",
    "    h_lrhs = hp_filter()(lrhs_inputs)\n",
    "    h_hrms = hp_filter()(hrms_inputs)\n",
    "    \n",
    "    re_h_lrhs = resize(hrms_size)(h_lrhs)\n",
    "    re_lrhs = resize(hrms_size)(lrhs_inputs)\n",
    "    \n",
    "    mixed = Concatenate()([re_h_lrhs, h_hrms])\n",
    "\n",
    "    mixed1 = Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu')(mixed)\n",
    "    \n",
    "    x = mixed1\n",
    "    for i in range(4):\n",
    "        x = conv_block(x, str(i))\n",
    "    \n",
    "    x = Conv2D(lrhs_size[2], (3, 3), strides=(1, 1), padding='same', name='model1_last1')(x)\n",
    "    \n",
    "    last = Add()([x, re_lrhs])\n",
    "    \n",
    "    model = Model(inputs = [lrhs_inputs, hrms_inputs], outputs = last)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr = 5e-4), loss = 'mae', metrics=[psnr])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def PanNet(hrms, lrhs, sensor = None):\n",
    "    \"\"\"\n",
    "    this is an zero-shot learning method with deep learning (PanNet)\n",
    "    hrms: numpy array with MXNXc\n",
    "    lrhs: numpy array with mxnxC\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "    \n",
    "    M, N, c = hrms.shape\n",
    "    m, n, C = lrhs.shape\n",
    "\n",
    "    stride = 8\n",
    "    training_size=64#training patch size\n",
    "    testing_size=400#testing patch size\n",
    "    reconstructing_size=320#reconstructing patch size to avoid boundary effect\n",
    "    left_pad = (testing_size-reconstructing_size)//2\n",
    "    \n",
    "    '''\n",
    "        testing\n",
    "    ---------------\n",
    "    |     rec     |\n",
    "    |   -------   |\n",
    "    |   |     |   |\n",
    "    |   |     |   |\n",
    "    |   -------   |\n",
    "    |             |\n",
    "    ---------------\n",
    "    |pad|\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    train_hrhs_all = []\n",
    "    train_hrms_all = []\n",
    "    train_lrhs_all = []\n",
    "    \n",
    "    used_hrhs = lrhs\n",
    "    used_lrhs = lrhs\n",
    "    \n",
    "    used_lrhs, used_hrms = downgrade_images(used_lrhs, hrms, ratio, sensor=sensor)\n",
    "    \n",
    "    print(used_lrhs.shape, used_hrms.shape)\n",
    "    \n",
    "    \"\"\"crop images\"\"\"\n",
    "    print('croping images...')\n",
    "    \n",
    "    for j in range(0, used_hrms.shape[0]-training_size, stride):\n",
    "        for k in range(0, used_hrms.shape[1]-training_size, stride):\n",
    "            \n",
    "            temp_hrhs = used_hrhs[j:j+training_size, k:k+training_size, :]\n",
    "            temp_hrms = used_hrms[j:j+training_size, k:k+training_size, :]\n",
    "            temp_lrhs = used_lrhs[int(j/4):int((j+training_size)/4), int(k/4):int((k+training_size)/4), :]\n",
    "            \n",
    "            train_hrhs_all.append(temp_hrhs)\n",
    "            train_hrms_all.append(temp_hrms)\n",
    "            train_lrhs_all.append(temp_lrhs)\n",
    "            \n",
    "    train_hrhs_all = np.array(train_hrhs_all, dtype='float16')\n",
    "    train_hrms_all = np.array(train_hrms_all, dtype='float16')\n",
    "    train_lrhs_all = np.array(train_lrhs_all, dtype='float16')\n",
    "    \n",
    "    index = [i for i in range(train_hrhs_all.shape[0])]\n",
    "#    random.seed(2020)\n",
    "    random.shuffle(index)\n",
    "    train_hrhs = train_hrhs_all[index, :, :, :]\n",
    "    train_hrms= train_hrms_all[index, :, :, :]\n",
    "    train_lrhs = train_lrhs_all[index, :, :, :]\n",
    "    \n",
    "    print(train_hrhs.shape, train_hrms.shape, train_lrhs.shape)\n",
    "    \n",
    "    \"\"\"train net\"\"\"\n",
    "    print('training...')\n",
    "    \n",
    "    def lr_schedule(epoch):\n",
    "        \"\"\"Learning Rate Schedule\n",
    "    \n",
    "        # Arguments\n",
    "            epoch (int): The number of epochs\n",
    "    \n",
    "        # Returns\n",
    "            lr (float32): learning rate\n",
    "        \"\"\"\n",
    "        lr = 5e-4\n",
    "        if epoch > 40:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 20:\n",
    "            lr *= 1e-1\n",
    "        return lr\n",
    "    \n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "    checkpoint = ModelCheckpoint(filepath='./weights/PANNET_model.h5',\n",
    "                             monitor='val_psnr',\n",
    "                             mode='max',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "    callbacks = [lr_scheduler, checkpoint]\n",
    "    \n",
    "    model = pannet(lrhs_size=(int(training_size/ratio), int(training_size/ratio), C), hrms_size=(training_size, training_size, c))\n",
    "        \n",
    "    model.fit( x=[train_lrhs, train_hrms],\n",
    "                y=train_hrhs,\n",
    "                validation_split=0.1,\n",
    "                batch_size=32,\n",
    "                epochs=50,\n",
    "                verbose=1,\n",
    "                callbacks=callbacks)\n",
    "    \n",
    "    model = pannet(lrhs_size=(int(testing_size/ratio), int(testing_size/ratio), C), hrms_size=(testing_size, testing_size, c))\n",
    "    \n",
    "    model.load_weights('./weights/PANNET_model.h5')\n",
    "    \n",
    "    \"\"\"eval\"\"\"\n",
    "    print('evaling...')\n",
    "        \n",
    "    used_lrhs = np.expand_dims(lrhs, 0)\n",
    "    used_hrms = np.expand_dims(hrms, 0)\n",
    "    \n",
    "    new_M = min(M, m*ratio)\n",
    "    new_N = min(N, n*ratio)\n",
    "    \n",
    "    print('output image size:', new_M, new_N)\n",
    "    \n",
    "    test_label = np.zeros((new_M, new_N, C), dtype = 'uint8')\n",
    "    \n",
    "    used_lrhs = used_lrhs[:, :new_M//ratio, :new_N//ratio, :]\n",
    "    used_hrms = used_hrms[:, :new_M, :new_N, :]\n",
    "    \n",
    "    used_lrhs = np.pad(used_lrhs, ((0, 0), (left_pad//ratio, testing_size//ratio), (left_pad//ratio, testing_size//ratio), (0, 0)), mode='symmetric')\n",
    "    used_hrms = np.pad(used_hrms, ((0, 0), (left_pad, testing_size), (left_pad, testing_size), (0, 0)), mode='symmetric')\n",
    "    \n",
    "    for h in tqdm(range(0, new_M, reconstructing_size)):\n",
    "        for w in range(0, new_N, reconstructing_size):\n",
    "            temp_lrhs = used_lrhs[:,int(h/ratio):int((h+testing_size)/ratio), int(w/ratio):int((w+testing_size)/ratio), :]\n",
    "            temp_hrms = used_hrms[:, h:h+testing_size, w:w+testing_size, :]\n",
    "            \n",
    "            fake = model.predict([temp_lrhs, temp_hrms])\n",
    "            fake = np.clip(fake, 0, 1)\n",
    "            fake.shape=(testing_size, testing_size, C)\n",
    "            fake = fake[left_pad:(testing_size-left_pad), left_pad:(testing_size-left_pad)]\n",
    "            fake = np.uint8(fake*255)\n",
    "            \n",
    "            if h+testing_size>new_M:\n",
    "                fake = fake[:new_M-h, :, :]\n",
    "                \n",
    "            if w+testing_size>new_N:\n",
    "                fake = fake[:, :new_N-w, :]\n",
    "            \n",
    "            test_label[h:h+reconstructing_size, w:w+reconstructing_size]=fake\n",
    "            \n",
    "#    K.clear_session()\n",
    "#    gc.collect()\n",
    "#    del model\n",
    "    \n",
    "    return np.uint8(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cedce9",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f23183b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upsample_interp23\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA \u001b[38;5;28;01mas\u001b[39;00m princomp\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPCA\u001b[39m(pan, hs):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_interp23\n",
    "from sklearn.decomposition import PCA as princomp\n",
    "\n",
    "def PCA(pan, hs):\n",
    "\n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    image_hr = pan\n",
    "    \n",
    "    #upsample\n",
    "    u_hs = upsample_interp23(hs, ratio)\n",
    "    \n",
    "    p = princomp(n_components=C)\n",
    "    pca_hs = p.fit_transform(np.reshape(u_hs, (M*N, C)))\n",
    "    \n",
    "    pca_hs = np.reshape(pca_hs, (M, N, C))\n",
    "    \n",
    "    I = pca_hs[:, :, 0]\n",
    "    \n",
    "    image_hr = (image_hr - np.mean(image_hr))*np.std(I, ddof=1)/np.std(image_hr, ddof=1)+np.mean(I)\n",
    "    \n",
    "    pca_hs[:, :, 0] = image_hr[:, :, 0]\n",
    "    \n",
    "    I_PCA = p.inverse_transform(pca_hs)\n",
    "    \n",
    "    #equalization\n",
    "    I_PCA = I_PCA-np.mean(I_PCA, axis=(0, 1))+np.mean(u_hs)\n",
    "    \n",
    "    #adjustment\n",
    "    I_PCA[I_PCA<0]=0\n",
    "    I_PCA[I_PCA>1]=1\n",
    "    \n",
    "    return np.uint8(I_PCA*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2f139",
   "metadata": {},
   "source": [
    "# PNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d327aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Concatenate, Conv2D, Input\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras import backend as K\n",
    "import os\n",
    "import random\n",
    "#from utils import upsample_interp23, downgrade_images\n",
    "import gc\n",
    "\n",
    "def psnr(y_true, y_pred):\n",
    "    \"\"\"Peak signal-to-noise ratio averaged over samples and channels.\"\"\"\n",
    "    mse = K.mean(K.square(y_true*255 - y_pred*255), axis=(-3, -2, -1))\n",
    "    return K.mean(20 * K.log(255 / K.sqrt(mse)) / np.log(10))\n",
    "\n",
    "def pnn_net(lrhs_size=(32, 32, 3), hrms_size = (32, 32, 1)):\n",
    "    \n",
    "    lrhs_inputs = Input(lrhs_size)\n",
    "    hrms_inputs = Input(hrms_size)\n",
    "    \n",
    "    mixed = Concatenate()([lrhs_inputs, hrms_inputs])\n",
    "\n",
    "    mixed1 = Conv2D(64, (9, 9), strides=(1, 1), padding='same', activation='relu')(mixed)\n",
    "\n",
    "    mixed1 = Conv2D(32, (5, 5), strides=(1, 1), padding='same', activation='relu')(mixed1)\n",
    "    \n",
    "    c6 = Conv2D(lrhs_size[2], (5, 5), strides=(1, 1), padding='same', activation='relu', name='model1_last1')(mixed1)\n",
    "    \n",
    "    model = Model(inputs = [lrhs_inputs, hrms_inputs], outputs = c6)\n",
    "\n",
    "    model.compile(optimizer =Adam(lr = 5e-4), loss = 'mse', metrics=[psnr])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def PNN(hrms, lrhs, sensor = None):\n",
    "    \"\"\"\n",
    "    this is an zero-shot learning method with deep learning (PNN)\n",
    "    hrms: numpy array with MXNXc\n",
    "    lrhs: numpy array with mxnxC\n",
    "    \"\"\"\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "    \n",
    "    M, N, c = hrms.shape\n",
    "    m, n, C = lrhs.shape\n",
    "    \n",
    "    stride = 8\n",
    "    training_size=32#training patch size\n",
    "    testing_size=400#testing patch size\n",
    "    reconstructing_size=320#reconstructing patch size\n",
    "    left_pad = (testing_size-reconstructing_size)//2\n",
    "    \n",
    "\n",
    "    '''\n",
    "        testing\n",
    "    ---------------\n",
    "    |     rec     |\n",
    "    |   -------   |\n",
    "    |   |     |   |\n",
    "    |   |     |   |\n",
    "    |   -------   |\n",
    "    |             |\n",
    "    ---------------\n",
    "    |pad|\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    train_hrhs_all = []\n",
    "    train_hrms_all = []\n",
    "    train_lrhs_all = []\n",
    "    \n",
    "    used_hrhs = lrhs\n",
    "    used_lrhs = lrhs\n",
    "    \n",
    "    used_lrhs, used_hrms = downgrade_images(used_lrhs, hrms, ratio, sensor=sensor)\n",
    "    \n",
    "    print(used_lrhs.shape, used_hrms.shape)\n",
    "    \n",
    "    used_lrhs = upsample_interp23(used_lrhs, ratio)\n",
    "    \n",
    "    \"\"\"crop images\"\"\"\n",
    "    print('croping images...')\n",
    "    \n",
    "    for j in range(0, used_hrms.shape[0]-training_size, stride):\n",
    "        for k in range(0, used_hrms.shape[1]-training_size, stride):\n",
    "            \n",
    "            temp_hrhs = used_hrhs[j:j+training_size, k:k+training_size, :]\n",
    "            temp_hrms = used_hrms[j:j+training_size, k:k+training_size, :]\n",
    "            temp_lrhs = used_lrhs[j:j+training_size, k:k+training_size, :]\n",
    "            \n",
    "            train_hrhs_all.append(temp_hrhs)\n",
    "            train_hrms_all.append(temp_hrms)\n",
    "            train_lrhs_all.append(temp_lrhs)\n",
    "            \n",
    "    train_hrhs_all = np.array(train_hrhs_all, dtype='float16')\n",
    "    train_hrms_all = np.array(train_hrms_all, dtype='float16')\n",
    "    train_lrhs_all = np.array(train_lrhs_all, dtype='float16')\n",
    "    \n",
    "    index = [i for i in range(train_hrhs_all.shape[0])]\n",
    "#    random.seed(2020)\n",
    "    random.shuffle(index)\n",
    "    train_hrhs = train_hrhs_all[index, :, :, :]\n",
    "    train_hrms= train_hrms_all[index, :, :, :]\n",
    "    train_lrhs = train_lrhs_all[index, :, :, :]\n",
    "    \n",
    "    print(train_hrhs.shape, train_hrms.shape, train_lrhs.shape)\n",
    "    \n",
    "    \"\"\"train net\"\"\"\n",
    "    print('training...')\n",
    "    \n",
    "    def lr_schedule(epoch):\n",
    "        \"\"\"Learning Rate Schedule\n",
    "    \n",
    "        # Arguments\n",
    "            epoch (int): The number of epochs\n",
    "    \n",
    "        # Returns\n",
    "            lr (float32): learning rate\n",
    "        \"\"\"\n",
    "        lr = 5e-4\n",
    "        if epoch > 40:\n",
    "            lr *= 1e-2\n",
    "        elif epoch > 20:\n",
    "            lr *= 1e-1\n",
    "        return lr\n",
    "    \n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "    checkpoint = ModelCheckpoint(filepath='./weights/PNN_model.h5',\n",
    "                             monitor='val_psnr',\n",
    "                             mode='max',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "    callbacks = [lr_scheduler, checkpoint]\n",
    "    \n",
    "    model = pnn_net(lrhs_size=(training_size, training_size, C), hrms_size=(training_size, training_size, c))\n",
    "        \n",
    "    model.fit( x=[train_lrhs, train_hrms],\n",
    "                y=train_hrhs,\n",
    "                validation_split=0.33,\n",
    "                batch_size=32,\n",
    "                epochs=50,\n",
    "                verbose=1,\n",
    "                callbacks=callbacks)\n",
    "    \n",
    "    model = pnn_net(lrhs_size=(testing_size, testing_size, C), hrms_size=(testing_size, testing_size, c))\n",
    "    \n",
    "    model.load_weights('./weights/PNN_model.h5')\n",
    "    \n",
    "    \"\"\"eval\"\"\"\n",
    "    print('evaling...')\n",
    "    \n",
    "    new_M = min(M, m*ratio)\n",
    "    new_N = min(N, n*ratio)\n",
    "    \n",
    "    print('output image size:', new_M, new_N)\n",
    "    \n",
    "    test_label = np.zeros((new_M, new_N, C), dtype = 'uint8')\n",
    "    \n",
    "    used_lrhs = lrhs[:new_M//ratio, :new_N//ratio, :]\n",
    "    used_hrms = hrms[:new_M, :new_N, :]\n",
    "    \n",
    "    used_lrhs = upsample_interp23(used_lrhs, ratio)\n",
    "    \n",
    "    used_lrhs = np.expand_dims(used_lrhs, 0)\n",
    "    used_hrms = np.expand_dims(used_hrms, 0)\n",
    "    \n",
    "    used_lrhs = np.pad(used_lrhs, ((0, 0), (left_pad, testing_size), (left_pad, testing_size), (0, 0)), mode='symmetric')\n",
    "    used_hrms = np.pad(used_hrms, ((0, 0), (left_pad, testing_size), (left_pad, testing_size), (0, 0)), mode='symmetric')\n",
    "    \n",
    "    for h in tqdm(range(0, new_M, reconstructing_size)):\n",
    "        for w in range(0, new_N, reconstructing_size):\n",
    "            temp_lrhs = used_lrhs[:, h:h+testing_size, w:w+testing_size, :]\n",
    "            temp_hrms = used_hrms[:, h:h+testing_size, w:w+testing_size, :]\n",
    "            \n",
    "            fake = model.predict([temp_lrhs, temp_hrms])\n",
    "            fake = np.clip(fake, 0, 1)\n",
    "            fake.shape=(testing_size, testing_size, C)\n",
    "            fake = fake[left_pad:(testing_size-left_pad), left_pad:(testing_size-left_pad)]\n",
    "            fake = np.uint8(fake*255)\n",
    "            \n",
    "            if h+testing_size>new_M:\n",
    "                fake = fake[:new_M-h, :, :]\n",
    "                \n",
    "            if w+testing_size>new_N:\n",
    "                fake = fake[:, :new_N-w, :]\n",
    "            \n",
    "            test_label[h:h+reconstructing_size, w:w+reconstructing_size]=fake\n",
    "    \n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    del model\n",
    "    \n",
    "    return np.uint8(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c7979",
   "metadata": {},
   "source": [
    "# SFIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from utils import upsample_interp23\n",
    "from scipy import signal\n",
    "\n",
    "def SFIM(pan, hs):\n",
    "\n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    #upsample\n",
    "    u_hs = upsample_interp23(hs, ratio)\n",
    "    \n",
    "    if np.mod(ratio, 2)==0:\n",
    "        ratio = ratio + 1\n",
    "        \n",
    "    pan = np.tile(pan, (1, 1, C))\n",
    "    \n",
    "    pan = (pan - np.mean(pan, axis=(0, 1)))*(np.std(u_hs, axis=(0, 1), ddof=1)/np.std(pan, axis=(0, 1), ddof=1))+np.mean(u_hs, axis=(0, 1))\n",
    "    \n",
    "    kernel = np.ones((ratio, ratio))\n",
    "    kernel = kernel/np.sum(kernel)\n",
    "    \n",
    "    I_SFIM = np.zeros((M, N, C))\n",
    "    for i in range(C):\n",
    "        lrpan = signal.convolve2d(pan[:, :, i], kernel, mode='same', boundary = 'wrap')\n",
    "        I_SFIM[:, :, i] = u_hs[:, :, i]*pan[:, :, i]/(lrpan+1e-8)\n",
    "\n",
    "    #adjustment\n",
    "    I_SFIM[I_SFIM<0]=0\n",
    "    I_SFIM[I_SFIM>1]=1    \n",
    "    \n",
    "    return np.uint8(I_SFIM*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d1cc6",
   "metadata": {},
   "source": [
    "# Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26bda0f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pywt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#from utils import upsample_interp23\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpywt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mWavelet\u001b[39m(pan, hs):\n\u001b[1;32m      7\u001b[0m     M, N, c \u001b[38;5;241m=\u001b[39m pan\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pywt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from utils import upsample_interp23\n",
    "import pywt\n",
    "\n",
    "def Wavelet(pan, hs):\n",
    "\n",
    "    M, N, c = pan.shape\n",
    "    m, n, C = hs.shape\n",
    "    \n",
    "    ratio = int(np.round(M/m))\n",
    "        \n",
    "    print('get sharpening ratio: ', ratio)\n",
    "    assert int(np.round(M/m)) == int(np.round(N/n))\n",
    "    \n",
    "    #upsample\n",
    "    u_hs = upsample_interp23(hs, ratio)\n",
    "    \n",
    "    pan = np.squeeze(pan)\n",
    "    pc = pywt.wavedec2(pan, 'haar', level=2)\n",
    "    \n",
    "    rec=[]\n",
    "    for i in range(C):\n",
    "        temp_dec = pywt.wavedec2(u_hs[:, :, i], 'haar', level=2)\n",
    "        \n",
    "        pc[0] = temp_dec[0]\n",
    "        \n",
    "        temp_rec = pywt.waverec2(pc, 'haar')\n",
    "        temp_rec = np.expand_dims(temp_rec, -1)\n",
    "        rec.append(temp_rec)\n",
    "        \n",
    "    I_Wavelet = np.concatenate(rec, axis=-1)\n",
    "    \n",
    "    #adjustment\n",
    "    I_Wavelet[I_Wavelet<0]=0\n",
    "    I_Wavelet[I_Wavelet>1]=1\n",
    "    \n",
    "    return np.uint8(I_Wavelet*255)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16da024",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa948e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "\n",
    "def sam(img1, img2):\n",
    "    \"\"\"SAM for 3D image, shape (H, W, C); uint or float[0, 1]\"\"\"\n",
    "    if not img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    assert img1.ndim == 3 and img1.shape[2] > 1, \"image n_channels should be greater than 1\"\n",
    "    img1_ = img1.astype(np.float64)\n",
    "    img2_ = img2.astype(np.float64)\n",
    "    inner_product = (img1_ * img2_).sum(axis=2)\n",
    "    img1_spectral_norm = np.sqrt((img1_**2).sum(axis=2))\n",
    "    img2_spectral_norm = np.sqrt((img2_**2).sum(axis=2))\n",
    "    # numerical stability\n",
    "    cos_theta = (inner_product / (img1_spectral_norm * img2_spectral_norm + np.finfo(np.float64).eps)).clip(min=0, max=1)\n",
    "    return np.mean(np.arccos(cos_theta))\n",
    "\n",
    "\n",
    "def psnr(img1, img2, dynamic_range=255):\n",
    "    \"\"\"PSNR metric, img uint8 if 225; uint16 if 2047\"\"\"\n",
    "    if not  img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    img1_ = img1.astype(np.float64)\n",
    "    img2_ = img2.astype(np.float64)\n",
    "    mse = np.mean((img1_ - img2_)**2)\n",
    "    if mse <= 1e-10:\n",
    "        return np.inf\n",
    "    return 20 * np.log10(dynamic_range / (np.sqrt(mse) + np.finfo(np.float64).eps))\n",
    "\n",
    "\n",
    "def scc(img1, img2):\n",
    "    \"\"\"SCC for 2D (H, W)or 3D (H, W, C) image; uint or float[0, 1]\"\"\"\n",
    "    if not  img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    img1_ = img1.astype(np.float64)\n",
    "    img2_ = img2.astype(np.float64)\n",
    "    if img1_.ndim == 2:\n",
    "        return np.corrcoef(img1_.reshape(1, -1), img2_.rehshape(1, -1))[0, 1]\n",
    "    elif img1_.ndim == 3:\n",
    "        #print(img1_[..., i].reshape[1, -1].shape)\n",
    "        #test = np.corrcoef(img1_[..., i].reshape[1, -1], img2_[..., i].rehshape(1, -1))\n",
    "        #print(type(test))\n",
    "        ccs = [np.corrcoef(img1_[..., i].reshape(1, -1), img2_[..., i].reshape(1, -1))[0, 1]\n",
    "               for i in range(img1_.shape[2])]\n",
    "        return np.mean(ccs)\n",
    "    else:\n",
    "        raise ValueError('Wrong input image dimensions.')\n",
    "\n",
    "\n",
    "def _qindex(img1, img2, block_size=8):\n",
    "    \"\"\"Q-index for 2D (one-band) image, shape (H, W); uint or float [0, 1]\"\"\"\n",
    "    assert block_size > 1, 'block_size shold be greater than 1!'\n",
    "    img1_ = img1.astype(np.float64)\n",
    "    img2_ = img2.astype(np.float64)\n",
    "    window = np.ones((block_size, block_size)) / (block_size**2)\n",
    "    # window_size = block_size**2\n",
    "    # filter, valid\n",
    "    pad_topleft = int(np.floor(block_size/2))\n",
    "    pad_bottomright = block_size - 1 - pad_topleft\n",
    "    mu1 = cv2.filter2D(img1_, -1, window)[pad_topleft:-pad_bottomright, pad_topleft:-pad_bottomright]\n",
    "    mu2 = cv2.filter2D(img2_, -1, window)[pad_topleft:-pad_bottomright, pad_topleft:-pad_bottomright]\n",
    "    mu1_sq = mu1**2\n",
    "    mu2_sq = mu2**2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    \n",
    "    sigma1_sq = cv2.filter2D(img1_**2, -1, window)[pad_topleft:-pad_bottomright, pad_topleft:-pad_bottomright] - mu1_sq\n",
    "    sigma2_sq = cv2.filter2D(img2_**2, -1, window)[pad_topleft:-pad_bottomright, pad_topleft:-pad_bottomright] - mu2_sq\n",
    "#    print(mu1_mu2.shape)\n",
    "    #print(sigma2_sq.shape)\n",
    "    sigma12 = cv2.filter2D(img1_ * img2_, -1, window)[pad_topleft:-pad_bottomright, pad_topleft:-pad_bottomright] - mu1_mu2\n",
    "\n",
    "    # all = 1, include the case of simga == mu == 0\n",
    "    qindex_map = np.ones(sigma12.shape)\n",
    "    # sigma == 0 and mu != 0\n",
    "    \n",
    "#    print(np.min(sigma1_sq + sigma2_sq), np.min(mu1_sq + mu2_sq))\n",
    "    \n",
    "    idx = ((sigma1_sq + sigma2_sq) < 1e-8) * ((mu1_sq + mu2_sq) >1e-8)\n",
    "    qindex_map[idx] = 2 * mu1_mu2[idx] / (mu1_sq + mu2_sq)[idx]\n",
    "    # sigma !=0 and mu == 0\n",
    "    idx = ((sigma1_sq + sigma2_sq) >1e-8) * ((mu1_sq + mu2_sq) < 1e-8)\n",
    "    qindex_map[idx] = 2 * sigma12[idx] / (sigma1_sq + sigma2_sq)[idx]\n",
    "    # sigma != 0 and mu != 0\n",
    "    idx = ((sigma1_sq + sigma2_sq) >1e-8) * ((mu1_sq + mu2_sq) >1e-8)\n",
    "    qindex_map[idx] =((2 * mu1_mu2[idx]) * (2 * sigma12[idx])) / (\n",
    "        (mu1_sq + mu2_sq)[idx] * (sigma1_sq + sigma2_sq)[idx])\n",
    "    \n",
    "#    print(np.mean(qindex_map))\n",
    "    \n",
    "#    idx = ((sigma1_sq + sigma2_sq) == 0) * ((mu1_sq + mu2_sq) != 0)\n",
    "#    qindex_map[idx] = 2 * mu1_mu2[idx] / (mu1_sq + mu2_sq)[idx]\n",
    "#    # sigma !=0 and mu == 0\n",
    "#    idx = ((sigma1_sq + sigma2_sq) != 0) * ((mu1_sq + mu2_sq) == 0)\n",
    "#    qindex_map[idx] = 2 * sigma12[idx] / (sigma1_sq + sigma2_sq)[idx]\n",
    "#    # sigma != 0 and mu != 0\n",
    "#    idx = ((sigma1_sq + sigma2_sq) != 0) * ((mu1_sq + mu2_sq) != 0)\n",
    "#    qindex_map[idx] =((2 * mu1_mu2[idx]) * (2 * sigma12[idx])) / (\n",
    "#        (mu1_sq + mu2_sq)[idx] * (sigma1_sq + sigma2_sq)[idx])\n",
    "    \n",
    "    return np.mean(qindex_map)\n",
    "\n",
    "\n",
    "def qindex(img1, img2, block_size=8):\n",
    "    \"\"\"Q-index for 2D (H, W) or 3D (H, W, C) image; uint or float [0, 1]\"\"\"\n",
    "    if not img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    if img1.ndim == 2:\n",
    "        return _qindex(img1, img2, block_size)\n",
    "    elif img1.ndim == 3:\n",
    "        qindexs = [_qindex(img1[..., i], img2[..., i], block_size) for i in range(img1.shape[2])]\n",
    "        return np.array(qindexs).mean()\n",
    "    else:\n",
    "        raise ValueError('Wrong input image dimensions.')\n",
    "\n",
    "\n",
    "def _ssim(img1, img2, dynamic_range=255):\n",
    "    \"\"\"SSIM for 2D (one-band) image, shape (H, W); uint8 if 225; uint16 if 2047\"\"\"\n",
    "    C1 = (0.01 * dynamic_range)**2\n",
    "    C2 = (0.03 * dynamic_range)**2\n",
    "    \n",
    "    img1_ = img1.astype(np.float64)\n",
    "    img2_ = img2.astype(np.float64)\n",
    "    kernel = cv2.getGaussianKernel(11, 1.5)  # kernel size 11\n",
    "    window = np.outer(kernel, kernel.transpose())\n",
    "    \n",
    "    mu1 = cv2.filter2D(img1_, -1, window)[5:-5, 5:-5]  # valid\n",
    "    mu2 = cv2.filter2D(img2_, -1, window)[5:-5, 5:-5]\n",
    "    mu1_sq = mu1**2\n",
    "    mu2_sq = mu2**2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = cv2.filter2D(img1_**2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
    "    sigma2_sq = cv2.filter2D(img2_**2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
    "    sigma12 = cv2.filter2D(img1_ * img2_, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
    "    \n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / (\n",
    "        (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "    return ssim_map.mean()\n",
    "\n",
    "\n",
    "def ssim(img1, img2, dynamic_range=255):\n",
    "    \"\"\"SSIM for 2D (H, W) or 3D (H, W, C) image; uint8 if 225; uint16 if 2047\"\"\"\n",
    "    if not img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    if img1.ndim == 2:\n",
    "        return _ssim(img1, img2, dynamic_range)\n",
    "    elif img1.ndim == 3:\n",
    "        ssims = [_ssim(img1[..., i], img2[..., i], dynamic_range) for i in range(img1.shape[2])]\n",
    "        return np.array(ssims).mean()\n",
    "    else:\n",
    "        raise ValueError('Wrong input image dimensions.')\n",
    "\n",
    "\n",
    "def ergas(img_fake, img_real, scale=4):\n",
    "    \"\"\"ERGAS for 2D (H, W) or 3D (H, W, C) image; uint or float [0, 1].\n",
    "    scale = spatial resolution of PAN / spatial resolution of MUL, default 4.\"\"\"\n",
    "    if not img_fake.shape == img_real.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    img_fake_ = img_fake.astype(np.float64)\n",
    "    img_real_ = img_real.astype(np.float64)\n",
    "    if img_fake_.ndim == 2:\n",
    "        mean_real = img_real_.mean()\n",
    "        mse = np.mean((img_fake_ - img_real_)**2)\n",
    "        return 100 / scale * np.sqrt(mse / (mean_real**2 + np.finfo(np.float64).eps))\n",
    "    elif img_fake_.ndim == 3:\n",
    "        means_real = img_real_.reshape(-1, img_real_.shape[2]).mean(axis=0)\n",
    "        mses = ((img_fake_ - img_real_)**2).reshape(-1, img_fake_.shape[2]).mean(axis=0)\n",
    "        return 100 / scale * np.sqrt((mses / (means_real**2 + np.finfo(np.float64).eps)).mean())\n",
    "    else:\n",
    "        raise ValueError('Wrong input image dimensions.')\n",
    "\n",
    "\n",
    "####################\n",
    "# observation model\n",
    "####################\n",
    "\n",
    "\n",
    "def gaussian2d(N, std):\n",
    "    t = np.arange(-(N - 1) // 2, (N + 2) // 2)\n",
    "    t1, t2 = np.meshgrid(t, t)\n",
    "    std = np.double(std)\n",
    "    w = np.exp(-0.5 * (t1 / std)**2) * np.exp(-0.5 * (t2 / std)**2) \n",
    "    return w\n",
    "\n",
    "\n",
    "def kaiser2d(N, beta):\n",
    "    t = np.arange(-(N - 1) // 2, (N + 2) // 2) / np.double(N - 1)\n",
    "    t1, t2 = np.meshgrid(t, t)\n",
    "    t12 = np.sqrt(t1 * t1 + t2 * t2)\n",
    "    w1 = np.kaiser(N, beta)\n",
    "    w = np.interp(t12, t, w1)\n",
    "    w[t12 > t[-1]] = 0\n",
    "    w[t12 < t[0]] = 0\n",
    "    return w\n",
    "\n",
    "\n",
    "def fir_filter_wind(Hd, w):\n",
    "    \"\"\"\n",
    "    compute fir (finite impulse response) filter with window method\n",
    "    Hd: desired freqeuncy response (2D)\n",
    "    w: window (2D)\n",
    "    \"\"\"\n",
    "    hd = np.rot90(np.fft.fftshift(np.rot90(Hd, 2)), 2)\n",
    "    h = np.fft.fftshift(np.fft.ifft2(hd))\n",
    "    h = np.rot90(h, 2)\n",
    "    h = h * w\n",
    "    h = h / np.sum(h)\n",
    "    return h\n",
    "\n",
    "\n",
    "def GNyq2win(GNyq, scale=4, N=41):\n",
    "    \"\"\"Generate a 2D convolutional window from a given GNyq\n",
    "    GNyq: Nyquist frequency\n",
    "    scale: spatial size of PAN / spatial size of MS\n",
    "    \"\"\"\n",
    "    #fir filter with window method\n",
    "    fcut = 1 / scale\n",
    "    alpha = np.sqrt(((N - 1) * (fcut / 2))**2 / (-2 * np.log(GNyq)))\n",
    "    H = gaussian2d(N, alpha)\n",
    "    Hd = H / np.max(H)\n",
    "    w = kaiser2d(N, 0.5)\n",
    "    h = fir_filter_wind(Hd, w)\n",
    "    return np.real(h)\n",
    "\n",
    "\n",
    "def mtf_resize(img, satellite='QuickBird', scale=4):\n",
    "    # satellite GNyq\n",
    "    scale = int(scale)\n",
    "    if satellite == 'QuickBird':\n",
    "        GNyq = [0.34, 0.32, 0.30, 0.22]  # Band Order: B,G,R,NIR\n",
    "        GNyqPan = 0.15\n",
    "    elif satellite == 'IKONOS':\n",
    "        GNyq = [0.26, 0.28, 0.29, 0.28]  # Band Order: B,G,R,NIR\n",
    "        GNyqPan = 0.17\n",
    "    else:\n",
    "        raise NotImplementedError('satellite: QuickBird or IKONOS')\n",
    "    # lowpass\n",
    "    img_ = img.squeeze()\n",
    "    img_ = img_.astype(np.float64)\n",
    "    if img_.ndim == 2:  # Pan\n",
    "        H, W = img_.shape\n",
    "        lowpass = GNyq2win(GNyqPan, scale, N=41)\n",
    "    elif img_.ndim == 3:  # MS\n",
    "        H, W, _ = img.shape\n",
    "        lowpass = [GNyq2win(gnyq, scale, N=41) for gnyq in GNyq]\n",
    "        lowpass = np.stack(lowpass, axis=-1)\n",
    "    img_ = ndimage.filters.correlate(img_, lowpass, mode='nearest')\n",
    "    # downsampling\n",
    "    output_size = (H // scale, W // scale)\n",
    "    img_ = cv2.resize(img_, dsize=output_size, interpolation=cv2.INTER_NEAREST)\n",
    "    return img_\n",
    "\n",
    "\n",
    "##################\n",
    "# No reference IQA\n",
    "##################\n",
    "\n",
    "\n",
    "def D_lambda(img_fake, img_lm, block_size=32, p=1):\n",
    "    \"\"\"Spectral distortion\n",
    "    img_fake, generated HRMS\n",
    "    img_lm, LRMS\"\"\"\n",
    "    assert img_fake.ndim == img_lm.ndim == 3, 'Images must be 3D!'\n",
    "    H_f, W_f, C_f = img_fake.shape\n",
    "    H_r, W_r, C_r = img_lm.shape\n",
    "    assert C_f == C_r, 'Fake and lm should have the same number of bands!'\n",
    "    # D_lambda\n",
    "    Q_fake = []\n",
    "    Q_lm = []\n",
    "    for i in range(C_f):\n",
    "        for j in range(i+1, C_f):\n",
    "            # for fake\n",
    "            band1 = img_fake[..., i]\n",
    "            band2 = img_fake[..., j]\n",
    "            Q_fake.append(_qindex(band1, band2, block_size=block_size))\n",
    "            # for real\n",
    "            band1 = img_lm[..., i]\n",
    "            band2 = img_lm[..., j]\n",
    "            Q_lm.append(_qindex(band1, band2, block_size=block_size))\n",
    "    Q_fake = np.array(Q_fake)\n",
    "    Q_lm = np.array(Q_lm)\n",
    "    D_lambda_index = (np.abs(Q_fake - Q_lm) ** p).mean()\n",
    "    return D_lambda_index ** (1/p)\n",
    "\n",
    "\n",
    "def D_s(img_fake, img_lm, pan, satellite='QuickBird', scale=4, block_size=32, q=1):\n",
    "    \"\"\"Spatial distortion\n",
    "    img_fake, generated HRMS\n",
    "    img_lm, LRMS\n",
    "    pan, HRPan\"\"\"\n",
    "    # fake and lm\n",
    "    assert img_fake.ndim == img_lm.ndim == 3, 'MS images must be 3D!'\n",
    "    H_f, W_f, C_f = img_fake.shape\n",
    "    H_r, W_r, C_r = img_lm.shape\n",
    "    assert H_f // H_r == W_f // W_r == scale, 'Spatial resolution should be compatible with scale'\n",
    "    assert C_f == C_r, 'Fake and lm should have the same number of bands!'\n",
    "    # fake and pan\n",
    "    assert pan.ndim == 3, 'Panchromatic image must be 3D!'\n",
    "    H_p, W_p, C_p = pan.shape\n",
    "    assert C_p == 1, 'size of 3rd dim of Panchromatic image must be 1'\n",
    "    assert H_f == H_p and W_f == W_p, \"Pan's and fake's spatial resolution should be the same\"\n",
    "    # get LRPan, 2D\n",
    "    pan_lr = mtf_resize(pan, satellite=satellite, scale=scale)\n",
    "    #print(pan_lr.shape)\n",
    "    # D_s\n",
    "    Q_hr = []\n",
    "    Q_lr = []\n",
    "    for i in range(C_f):\n",
    "        # for HR fake\n",
    "        band1 = img_fake[..., i]\n",
    "        band2 = pan[..., 0] # the input PAN is 3D with size=1 along 3rd dim\n",
    "        #print(band1.shape)\n",
    "        #print(band2.shape)\n",
    "        Q_hr.append(_qindex(band1, band2, block_size=block_size))\n",
    "        band1 = img_lm[..., i]\n",
    "        band2 = pan_lr  # this is 2D\n",
    "        #print(band1.shape)\n",
    "        #print(band2.shape)\n",
    "        Q_lr.append(_qindex(band1, band2, block_size=block_size))\n",
    "    Q_hr = np.array(Q_hr)\n",
    "    Q_lr = np.array(Q_lr)\n",
    "    D_s_index = (np.abs(Q_hr - Q_lr) ** q).mean()\n",
    "    return D_s_index ** (1/q)\n",
    "\n",
    "def qnr(img_fake, img_lm, pan, satellite='QuickBird', scale=4, block_size=32, p=1, q=1, alpha=1, beta=1):\n",
    "    \"\"\"QNR - No reference IQA\"\"\"\n",
    "    D_lambda_idx = D_lambda(img_fake, img_lm, block_size, p)\n",
    "    D_s_idx = D_s(img_fake, img_lm, pan, satellite, scale, block_size, q)\n",
    "    QNR_idx = (1 - D_lambda_idx) ** alpha * (1 - D_s_idx) ** beta\n",
    "    return QNR_idx\n",
    "\n",
    "\n",
    "def ref_evaluate(pred, gt):\n",
    "    #reference metrics\n",
    "    c_psnr = psnr(pred, gt)\n",
    "    c_ssim = ssim(pred, gt)\n",
    "    c_sam = sam(pred, gt)\n",
    "    c_ergas = ergas(pred, gt)\n",
    "    c_scc = scc(pred, gt)\n",
    "    c_q = qindex(pred, gt)\n",
    "\n",
    "    return [c_psnr, c_ssim, c_sam, c_ergas, c_scc, c_q]\n",
    "\n",
    "def no_ref_evaluate(pred, pan, hs):\n",
    "    #no reference metrics\n",
    "    c_D_lambda = D_lambda(pred, hs)\n",
    "    c_D_s = D_s(pred, hs, pan)\n",
    "    c_qnr = qnr(pred, hs, pan)\n",
    "    \n",
    "    return [c_D_lambda, c_D_s, c_qnr]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0961cb97",
   "metadata": {},
   "source": [
    "# demo_all_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f3f342",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signal\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "from methods.Bicubic import Bicubic\n",
    "from methods.Brovey import Brovey\n",
    "from methods.PCA import PCA\n",
    "from methods.IHS import IHS\n",
    "from methods.SFIM import SFIM\n",
    "from methods.GS import GS\n",
    "from methods.Wavelet import Wavelet\n",
    "from methods.MTF_GLP import MTF_GLP\n",
    "from methods.MTF_GLP_HPM import MTF_GLP_HPM\n",
    "from methods.GSA import GSA\n",
    "from methods.CNMF import CNMF\n",
    "from methods.GFPCA import GFPCA\n",
    "from methods.PNN import PNN\n",
    "from methods.PanNet import PanNet\n",
    "\n",
    "from metrics import ref_evaluate, no_ref_evaluate\n",
    "\n",
    "'''loading data'''\n",
    "original_msi = np.load('./images/GF2_BJ_mss.npy')\n",
    "original_pan = np.load('./images/GF2_BJ_pan.npy')\n",
    "\n",
    "'''normalization'''\n",
    "max_patch, min_patch = np.max(original_msi, axis=(0,1)), np.min(original_msi, axis=(0,1))\n",
    "original_msi = np.float32(original_msi-min_patch) / (max_patch - min_patch)\n",
    "\n",
    "max_patch, min_patch = np.max(original_pan, axis=(0,1)), np.min(original_pan, axis=(0,1))\n",
    "original_pan = np.float32(original_pan-min_patch) / (max_patch - min_patch)\n",
    "\n",
    "'''generating ms image with gaussian kernel'''\n",
    "sig = (1/(2*(2.772587)/4**2))**0.5\n",
    "kernel = np.multiply(cv2.getGaussianKernel(9, sig), cv2.getGaussianKernel(9,sig).T)\n",
    "new_lrhs = []\n",
    "for i in range(original_msi.shape[-1]):\n",
    "    temp = signal.convolve2d(original_msi[:,:, i], kernel, boundary='wrap',mode='same')\n",
    "    temp = np.expand_dims(temp, -1)\n",
    "    new_lrhs.append(temp)\n",
    "new_lrhs = np.concatenate(new_lrhs, axis=-1)\n",
    "used_ms = new_lrhs[0::4, 0::4, :]\n",
    "\n",
    "#'''generating ms image with bicubic interpolation'''\n",
    "#used_ms = cv2.resize(original_msi, (original_msi.shape[1]//4, original_msi.shape[0]//4), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "'''generating pan image with gaussian kernel'''\n",
    "used_pan = signal.convolve2d(original_pan, kernel, boundary='wrap',mode='same')\n",
    "used_pan = np.expand_dims(used_pan, -1)\n",
    "used_pan = used_pan[0::4, 0::4, :]\n",
    "\n",
    "#'''generating pan image with vitual spectral kernel'''\n",
    "#spectral_kernel = np.array([[0.1], [0.1], [0.4], [0.4]])\n",
    "#used_pan = np.dot(original_msi, spectral_kernel)\n",
    "\n",
    "#'''generating ms image with bicubic interpolation'''\n",
    "#used_pan = cv2.resize(original_pan, (original_pan.shape[1]//4, original_pan.shape[0]//4), interpolation=cv2.INTER_CUBIC)\n",
    "#used_pan = np.expand_dims(used_pan, -1)\n",
    "\n",
    "gt = np.uint8(255*original_msi)\n",
    "\n",
    "print('ms shape: ', used_ms.shape, 'pan shape: ', used_pan.shape)\n",
    "\n",
    "'''setting save parameters'''\n",
    "save_images = True\n",
    "save_channels = [0, 1, 2]#BGR-NIR for GF2\n",
    "save_dir='./results/'\n",
    "if save_images and (not os.path.isdir(save_dir)):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "'''evaluating all methods'''\n",
    "ref_results={}\n",
    "ref_results.update({'metrics: ':'  PSNR,     SSIM,   SAM,    ERGAS,  SCC,    Q'})\n",
    "no_ref_results={}\n",
    "no_ref_results.update({'metrics: ':'  D_lamda, D_s,    QNR'})\n",
    "\n",
    "'''Bicubic method'''\n",
    "print('evaluating Bicubic method')\n",
    "fused_image = Bicubic(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'Bicubic    ':temp_ref_results})\n",
    "no_ref_results.update({'Bicubic    ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'Bicubic.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "'''Brovey method'''\n",
    "print('evaluating Brovey method')\n",
    "fused_image = Brovey(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'Brovey     ':temp_ref_results})\n",
    "no_ref_results.update({'Brovey     ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'Brovey.tiff', fused_image[:, :, save_channels])\n",
    "    \n",
    "'''PCA method'''\n",
    "print('evaluating PCA method')\n",
    "fused_image = PCA(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'PCA        ':temp_ref_results})\n",
    "no_ref_results.update({'PCA        ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'PCA.tiff', fused_image[:, :, save_channels])\n",
    "    \n",
    "'''IHS method'''\n",
    "print('evaluating IHS method')\n",
    "fused_image = IHS(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'IHS        ':temp_ref_results})\n",
    "no_ref_results.update({'IHS        ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'IHS.tiff', fused_image[:, :, save_channels])\n",
    "    \n",
    "'''SFIM method'''\n",
    "print('evaluating SFIM method')\n",
    "fused_image = SFIM(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'SFIM       ':temp_ref_results})\n",
    "no_ref_results.update({'SFIM       ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'SFIM.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "'''GS method'''\n",
    "print('evaluating GS method')\n",
    "fused_image = GS(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'GS         ':temp_ref_results})\n",
    "no_ref_results.update({'GS         ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'GS.tiff', fused_image[:, :, save_channels])\n",
    "    \n",
    "'''Wavelet method'''\n",
    "print('evaluating Wavelet method')\n",
    "fused_image = Wavelet(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'Wavelet    ':temp_ref_results})\n",
    "no_ref_results.update({'Wavelet    ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'Wavelet.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "'''MTF_GLP method'''\n",
    "print('evaluating MTF_GLP method')\n",
    "fused_image = MTF_GLP(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'MTF_GLP    ':temp_ref_results})\n",
    "no_ref_results.update({'MTF_GLP    ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'MTF_GLP.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "'''MTF_GLP_HPM method'''\n",
    "print('evaluating MTF_GLP_HPM method')\n",
    "fused_image = MTF_GLP_HPM(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'MTF_GLP_HPM':temp_ref_results})\n",
    "no_ref_results.update({'MTF_GLP_HPM':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'MTF_GLP_HPM.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "'''GSA method'''\n",
    "print('evaluating GSA method')\n",
    "fused_image = GSA(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'GSA        ':temp_ref_results})\n",
    "no_ref_results.update({'GSA        ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'GSA.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "'''CNMF method'''\n",
    "print('evaluating CNMF method')\n",
    "fused_image = CNMF(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'CNMF       ':temp_ref_results})\n",
    "no_ref_results.update({'CNMF       ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'CNMF.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "'''GFPCA method'''\n",
    "print('evaluating GFPCA method')\n",
    "fused_image = GFPCA(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'GFPCA      ':temp_ref_results})\n",
    "no_ref_results.update({'GFPCA      ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'GFPCA.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "'''PNN method'''\n",
    "print('evaluating PNN method')\n",
    "fused_image = PNN(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'PNN        ':temp_ref_results})\n",
    "no_ref_results.update({'PNN        ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'PNN.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "'''PanNet method'''\n",
    "print('evaluating PanNet method')\n",
    "fused_image = PanNet(used_pan[:, :, :], used_ms[:, :, :])\n",
    "temp_ref_results = ref_evaluate(fused_image, gt)\n",
    "temp_no_ref_results = no_ref_evaluate(fused_image, np.uint8(used_pan*255), np.uint8(used_ms*255))\n",
    "ref_results.update({'PanNet     ':temp_ref_results})\n",
    "no_ref_results.update({'PanNet     ':temp_no_ref_results})\n",
    "#save\n",
    "if save_images:\n",
    "    cv2.imwrite(save_dir+'PanNet.tiff', fused_image[:, :, save_channels])\n",
    "\n",
    "''''print result'''\n",
    "print('################## reference comparision #######################')\n",
    "for index, i in enumerate(ref_results):\n",
    "    if index == 0:\n",
    "        print(i, ref_results[i])\n",
    "    else:    \n",
    "        print(i, [round(j, 4) for j in ref_results[i]])\n",
    "print('################## reference comparision #######################')\n",
    "      \n",
    "      \n",
    "print('################## no reference comparision ####################')\n",
    "for index, i in enumerate(no_ref_results):\n",
    "    if index == 0:\n",
    "        print(i, no_ref_results[i])\n",
    "    else:    \n",
    "        print(i, [round(j, 4) for j in no_ref_results[i]])\n",
    "print('################## no reference comparision ####################')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601cd664",
   "metadata": {},
   "source": [
    "# demo_pansharpening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc251f09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msio\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import scipy.io as sio\n",
    "\n",
    "from methods.Bicubic import Bicubic\n",
    "from methods.Brovey import Brovey\n",
    "from methods.PCA import PCA\n",
    "from methods.IHS import IHS\n",
    "from methods.SFIM import SFIM\n",
    "from methods.GS import GS\n",
    "from methods.Wavelet import Wavelet\n",
    "from methods.MTF_GLP import MTF_GLP\n",
    "from methods.MTF_GLP_HPM import MTF_GLP_HPM\n",
    "from methods.GSA import GSA\n",
    "from methods.CNMF import CNMF\n",
    "from methods.GFPCA import GFPCA\n",
    "from methods.PNN import PNN\n",
    "from methods.PanNet import PanNet\n",
    "\n",
    "#'''loading data'''\n",
    "#used_ms = np.load('./images/GF2_BJ_mss.npy')\n",
    "#used_pan = np.load('./images/GF2_BJ_pan.npy')\n",
    "#used_pan = np.expand_dims(used_pan, -1)\n",
    "\n",
    "data = sio.loadmat('./images/imgWV2.mat')\n",
    "used_ms = data['I_MS']\n",
    "used_pan = data['I_PAN']\n",
    "used_pan = np.expand_dims(used_pan, -1)\n",
    "\n",
    "'''normalization'''\n",
    "max_patch, min_patch = np.max(used_ms, axis=(0,1)), np.min(used_ms, axis=(0,1))\n",
    "used_ms = np.float32(used_ms-min_patch) / (max_patch - min_patch)\n",
    "max_patch, min_patch = np.max(used_pan, axis=(0,1)), np.min(used_pan, axis=(0,1))\n",
    "used_pan = np.float32(used_pan-min_patch) / (max_patch - min_patch)\n",
    "\n",
    "print('ms shape: ', used_ms.shape, 'pan shape: ', used_pan.shape)\n",
    "\n",
    "save_dir='./results/'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "'''here is the main function'''\n",
    "fused_image = GSA(used_pan[:, :, :], used_ms[:, :, :])\n",
    "\n",
    "cv2.imwrite(save_dir+'GSA.tiff', fused_image[:, :, [2, 3, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2b26e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
